{"name": "README", "type": "document", "content": "# ğŸ€ Basketball Reference Data Pipeline \n\nThis project builds a cross-platform data pipeline for collecting, cleaning, and analyzing NBA data from Basketball Reference. \n\nIt automatically crawls player, team, MVP, and championship data, stores it in a MySQL database, and prepares clean datasets (csv*.csv) for further statistical and visual analysis in Jupyter notebooks. \n\n---\n\n## ğŸ“‚ Repository Structure \n\n\nâ”œâ”€â”€ data/ # Raw and cleaned datasets (CSV)\nâ”‚ â”œâ”€â”€ all_players.csv\nâ”‚ â”œâ”€â”€ all_teams.csv\nâ”‚ â”œâ”€â”€ mvp_players.csv\nâ”‚ â”œâ”€â”€ nba_champs.csv\nâ”‚ â”œâ”€â”€ top50_players.csv\nâ”‚ â””â”€â”€ csv*.csv # Analysis-ready tables\nâ”‚\nâ”œâ”€â”€ scripts/ # Crawlers & data preparation scripts\nâ”‚ â”œâ”€â”€ champ_team_crawler.py\nâ”‚ â”œâ”€â”€ clean_extract.py\nâ”‚ â”œâ”€â”€ mvp_player_crawler.py\nâ”‚ â”œâ”€â”€ player_crawler.py\nâ”‚ â”œâ”€â”€ prepare_data.py\nâ”‚ â”œâ”€â”€ team_crawler_ali.py\nâ”‚ â””â”€â”€ top50_player_ali.py\nâ”‚\nâ”œâ”€â”€ query_final.sql # SQL queries for creating analysis tables\nâ”œâ”€â”€ init.py # Orchestrates crawling, cleaning & DB storage\nâ”œâ”€â”€ Visual_and_Statistical_test_functions.ipynb # Analysis notebook\nâ”œâ”€â”€ Analyzer_temp_update.ipynb # Experimental notebook\nâ”œâ”€â”€ explanation311.docx # Project notes\nâ”œâ”€â”€ database_init.json # MySQL connection info (excluded in .gitignore)\nâ””â”€â”€ .gitignore\n\n\n---\n\n## âš™ï¸ Installation \n\nThis project is cross-platform (Linux, macOS, Windows). \nYouâ€™ll need the following dependencies: \n\n- Python 3.9\n- MySQL server\n- pip packages (install via requirements.txt if you create one later) \n\nExample: \nbash\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\npython -m venv venv\nsource venv/bin/activate # (Windows: venv\\Scripts\\activate)\npip install -r requirements.txt\n\n\n---\n\n## ğŸ—„ï¸ Database Setup \n\nBefore running the pipeline, create a database_init.json file in the project root (not included in GitHub for privacy reasons): \n\njson\n{\n \"host\": \"your-mysql-host\",\n \"user\": \"your-mysql-username\",\n \"pass\": \"your-mysql-password\"\n}\n\n\nâš ï¸ Do not commit this file â€” it is listed in .gitignore. \n\n---\n\n## ğŸš€ Usage \n\n1. Ensure the database_init.json file is configured with your MySQL credentials. \n2. Run the initializer to populate and clean the database: \n\nbash\npython init.py\n\n\nWhat happens: \n- If required CSVs are missing, init.py calls the crawlers in scripts/ to fetch data. \n- Data is cleaned (clean_extract.py, prepare_data.py). \n- Cleaned data is inserted into your MySQL database. \n- Queries from query_final.sql are executed to create analysis tables (csv*.csv). \n\n3. Explore and analyze the data in the provided Jupyter notebooks: \n\nbash\njupyter notebook\n\n\n- Visual_and_Statistical_test_functions.ipynb â†’ Final analysis & visualization \n- Analyzer_temp_update.ipynb â†’ Experimental / temp analysis \n\n---\n\n## ğŸ“Š Data Sources \n\n- Basketball Reference \n\nCrawlers include: \n- player_crawler.py â†’ Player stats \n- mvp_player_crawler.py â†’ MVP winners \n- champ_team_crawler.py â†’ Championship teams \n- team_crawler_ali.py â†’ Team data \n- top50_player_ali.py â†’ Top 50 players \n\n---\n\n## ğŸ“ Notes \n\n- database_init.json must be created by the user with proper MySQL credentials. \n- Data in /data may already exist, but missing files will be auto-generated by crawlers. \n- The project is intended to run on any OS with Python + MySQL installed. \n\n---\n\n## ğŸ“Œ Future Improvements \n\n- Add Docker support for database + pipeline. \n- Improve crawler efficiency with async requests. \n- Create a requirements.txt file for easier dependency installation. \n\n---\n\n## ğŸ“œ License \n\nMIT License â€“ feel free to use, modify, and share. \n"}
