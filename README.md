{"name": "README", "type": "document", "content": "# 🏀 Basketball Reference Data Pipeline \n\nThis project builds a cross-platform data pipeline for collecting, cleaning, and analyzing NBA data from Basketball Reference. \n\nIt automatically crawls player, team, MVP, and championship data, stores it in a MySQL database, and prepares clean datasets (csv*.csv) for further statistical and visual analysis in Jupyter notebooks. \n\n---\n\n## 📂 Repository Structure \n\n\n├── data/ # Raw and cleaned datasets (CSV)\n│ ├── all_players.csv\n│ ├── all_teams.csv\n│ ├── mvp_players.csv\n│ ├── nba_champs.csv\n│ ├── top50_players.csv\n│ └── csv*.csv # Analysis-ready tables\n│\n├── scripts/ # Crawlers & data preparation scripts\n│ ├── champ_team_crawler.py\n│ ├── clean_extract.py\n│ ├── mvp_player_crawler.py\n│ ├── player_crawler.py\n│ ├── prepare_data.py\n│ ├── team_crawler_ali.py\n│ └── top50_player_ali.py\n│\n├── query_final.sql # SQL queries for creating analysis tables\n├── init.py # Orchestrates crawling, cleaning & DB storage\n├── Visual_and_Statistical_test_functions.ipynb # Analysis notebook\n├── Analyzer_temp_update.ipynb # Experimental notebook\n├── explanation311.docx # Project notes\n├── database_init.json # MySQL connection info (excluded in .gitignore)\n└── .gitignore\n\n\n---\n\n## ⚙️ Installation \n\nThis project is cross-platform (Linux, macOS, Windows). \nYou’ll need the following dependencies: \n\n- Python 3.9\n- MySQL server\n- pip packages (install via requirements.txt if you create one later) \n\nExample: \nbash\ngit clone https://github.com/your-username/your-repo.git\ncd your-repo\npython -m venv venv\nsource venv/bin/activate # (Windows: venv\\Scripts\\activate)\npip install -r requirements.txt\n\n\n---\n\n## 🗄️ Database Setup \n\nBefore running the pipeline, create a database_init.json file in the project root (not included in GitHub for privacy reasons): \n\njson\n{\n \"host\": \"your-mysql-host\",\n \"user\": \"your-mysql-username\",\n \"pass\": \"your-mysql-password\"\n}\n\n\n⚠️ Do not commit this file — it is listed in .gitignore. \n\n---\n\n## 🚀 Usage \n\n1. Ensure the database_init.json file is configured with your MySQL credentials. \n2. Run the initializer to populate and clean the database: \n\nbash\npython init.py\n\n\nWhat happens: \n- If required CSVs are missing, init.py calls the crawlers in scripts/ to fetch data. \n- Data is cleaned (clean_extract.py, prepare_data.py). \n- Cleaned data is inserted into your MySQL database. \n- Queries from query_final.sql are executed to create analysis tables (csv*.csv). \n\n3. Explore and analyze the data in the provided Jupyter notebooks: \n\nbash\njupyter notebook\n\n\n- Visual_and_Statistical_test_functions.ipynb → Final analysis & visualization \n- Analyzer_temp_update.ipynb → Experimental / temp analysis \n\n---\n\n## 📊 Data Sources \n\n- Basketball Reference \n\nCrawlers include: \n- player_crawler.py → Player stats \n- mvp_player_crawler.py → MVP winners \n- champ_team_crawler.py → Championship teams \n- team_crawler_ali.py → Team data \n- top50_player_ali.py → Top 50 players \n\n---\n\n## 📝 Notes \n\n- database_init.json must be created by the user with proper MySQL credentials. \n- Data in /data may already exist, but missing files will be auto-generated by crawlers. \n- The project is intended to run on any OS with Python + MySQL installed. \n\n---\n\n## 📌 Future Improvements \n\n- Add Docker support for database + pipeline. \n- Improve crawler efficiency with async requests. \n- Create a requirements.txt file for easier dependency installation. \n\n---\n\n## 📜 License \n\nMIT License – feel free to use, modify, and share. \n"}
