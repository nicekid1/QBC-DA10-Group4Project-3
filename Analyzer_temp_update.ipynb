{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fc7f71cb",
   "metadata": {
    "id": "fc7f71cb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import warnings\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "02c7e55f",
   "metadata": {
    "id": "02c7e55f"
   },
   "outputs": [],
   "source": [
    "class Analyzer:\n",
    "    def __init__(self):\n",
    "        self.A = None\n",
    "        self.B = None\n",
    "        self.paired = None\n",
    "        self.testtype = None\n",
    "        self.result = None\n",
    "\n",
    "    def format_prepare(self, data, column=None):\n",
    "        \"\"\"\n",
    "            This method should check for the data to be of right format.\n",
    "            For example if the data is a DataFrame, it should change the data to Series or numpy array.\n",
    "        \"\"\"\n",
    "        if isinstance(data, np.ndarray):\n",
    "            if data.ndim > 1:\n",
    "                return data.flatten()\n",
    "            return data\n",
    "\n",
    "        elif isinstance(data, pd.Series):\n",
    "            return data.values\n",
    "\n",
    "        elif isinstance(data, pd.DataFrame):\n",
    "            if column is None:\n",
    "                if data.shape[1] == 1:\n",
    "                    return data.iloc[:, 0].values\n",
    "                else:\n",
    "                    raise ValueError(\"DataFrame has multiple columns. Please specify 'column'.\")\n",
    "            else:\n",
    "                if column not in data.columns:\n",
    "                    raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
    "                return data[column].values\n",
    "        elif isinstance(data, list):\n",
    "            return np.array(data)\n",
    "\n",
    "        else:\n",
    "            raise TypeError(\"Unsupported data type. Use list, numpy array, pandas Series, or DataFrame.\")\n",
    "\n",
    "\n",
    "    def fit(self, a, b, column_a=None, column_b=None, paired=False):\n",
    "        \"\"\"\n",
    "            This method assigns the data after fixing the format and check for their lengths.\n",
    "        \"\"\"\n",
    "        format_checked_a = self.format_prepare(a, column_a)\n",
    "        format_checked_b = self.format_prepare(b, column_b)\n",
    "\n",
    "        if len(format_checked_a) != len(format_checked_b):\n",
    "            raise ValueError(\"x and y must have the same length.\")\n",
    "        self.A = format_checked_a\n",
    "        self.B = format_checked_b\n",
    "        self.paired = paired\n",
    "\n",
    "    def summary_check(self):\n",
    "        return {\n",
    "            \"x_shape\": self.A.shape,\n",
    "            \"y_shape\": self.B.shape,\n",
    "            \"x_dtype\": self.A.dtype,\n",
    "            \"y_dtype\": self.B.dtype\n",
    "        }\n",
    "\n",
    "    def analysis_starter(self, alpha=0.05):\n",
    "        \"\"\"\"\n",
    "            This method is the analysis starter. it has to check the data for normality and\n",
    "            also should distinguish between independent and paired data.\n",
    "            after all the necessary operation before tests, __do_test is called to do the test.\n",
    "        \"\"\"\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception(\"A and B are empty. You must specify data group a and b with object.fit(a, b).\")\n",
    "\n",
    "        shapiro_a = stats.shapiro(self.A)\n",
    "        shapiro_b = stats.shapiro(self.B)\n",
    "\n",
    "        normal_a = shapiro_a.pvalue > alpha\n",
    "        normal_b = shapiro_b.pvalue > alpha\n",
    "        normal = normal_a & normal_b\n",
    "\n",
    "        if self.paired:\n",
    "            if normal:\n",
    "                self.testtype = \"paired_ttest\"\n",
    "            else:\n",
    "                self.testtype = \"wilcoxon\"\n",
    "        else:\n",
    "            if normal:\n",
    "                levene_test = stats.levene(self.A, self.B)\n",
    "                equal_var = levene_test.pvalue > alpha\n",
    "                self.testtype = \"independent_ttest_equalvar\" if equal_var else \"independent_ttest_welch\"\n",
    "            else:\n",
    "                self.testtype = \"mannwhitneyu\"\n",
    "\n",
    "    def __initial_test(self, test):\n",
    "        if test == 't-test':\n",
    "            self.result = stats.ttest_ind(self.A, self.B)\n",
    "        elif test == 'mann-whitney-u':\n",
    "            self.result = stats.mannwhitneyu(self.A, self.B)\n",
    "\n",
    "    def interpret(self):\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception('A and B are empty. You must specify data group a and b with object.fit(a, b).')\n",
    "        score, p_val = self.result\n",
    "        if p_val < 0.05:\n",
    "            print('Rejecting H0 hypothesis')\n",
    "        else:\n",
    "            print('Cannot reject H0.')\n",
    "\n",
    "    def __plot_single_numeric(data, name):\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(16, 7), gridspec_kw={'height_ratios':(.85, .15)})\n",
    "        sns.histplot(data, kde=True, ax=ax[0, 0], color='#55A868')\n",
    "        sns.boxplot(data, orient='h', ax=ax[1, 0], color=\"#5583A8\")\n",
    "        counts, bin_edges = np.histogram(data, bins=10, density = True)\n",
    "        pdf = counts / (sum(counts))\n",
    "        cdf = np.cumsum(pdf)\n",
    "        ax[1, 1] = plt.subplot(122)\n",
    "        plt.plot(bin_edges[1:], pdf, label='PDF')\n",
    "        plt.plot(bin_edges[1:], cdf, label='CDF')\n",
    "        plt.legend()\n",
    "        ax[0, 0].set_xticklabels([])\n",
    "        ax[1, 0].set_yticklabels([])\n",
    "        ax[0, 0].set_xlabel('')\n",
    "        ax[0, 0].set_ylabel('Count')\n",
    "        fig.suptitle(name, fontsize=30)\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def __plot_groups_numerical(self):\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        #TODO: the structure of data must be specified for this function.\n",
    "        # # First plot: A histogram of two groups\n",
    "        # sns.histplot(data=[self.A, self.B]],\n",
    "        #             x=num_col, hue=target_col,\n",
    "        #             kde=True,\n",
    "        #             ax=ax[0, 0],\n",
    "        #             element='step',\n",
    "        #             palette='Pastel1',\n",
    "        #             alpha=0.7)\n",
    "        # ax[0, 0].set_xlabel('')\n",
    "        # legend = ax[0, 0].get_legend()\n",
    "        # handles = legend.legend_handles\n",
    "        # ax[0, 0].legend(handles, ['No', 'Yes'], title=target_col)\n",
    "\n",
    "        # # Second plot: Box plots of two Groups\n",
    "        # sns.boxplot(data = df, x=target_col, y=num_col, palette='Pastel1', ax=ax[0, 1])\n",
    "        # ax[0, 1].set_xticklabels([])\n",
    "        # ax[0, 1].set_xlabel('')\n",
    "        # # Third plot: Violen plot\n",
    "        # sns.violinplot(data=df, x=target_col, y=num_col, palette='Pastel1', ax=ax[1, 1])\n",
    "        # ax[1, 1].set_xticks([0, 1], ['No', 'Yes'])\n",
    "        # # Forth plot: Q-Q plot\n",
    "        # stats.probplot(df[num_col], plot=ax[1, 0])\n",
    "        # fig.tight_layout()\n",
    "        # plt.show()\n",
    "\n",
    "    def visual_description(self):\n",
    "        \"\"\"\n",
    "        This method draws related plots. it can be used alongside interpret for better understanding or\n",
    "        it can be used seperately to give a visual on two groups of data.\n",
    "        \"\"\"\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception('A and B are empty. You must specify data group a and b with object.fit(a, b).')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9485304",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: core test runner & effect sizes (add INSIDE Analyzer class)\n",
    "\n",
    "def run_test(self, alpha: float = 0.05):\n",
    "    \"\"\"\n",
    "    Run the appropriate statistical test based on `self.testtype`.\n",
    "    If `self.testtype` is not set, it will call `analysis_starter` first.\n",
    "    Stores the result in `self.result` as a dict.\n",
    "    \"\"\"\n",
    "    if self.A is None or self.B is None:\n",
    "        raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
    "\n",
    "    # Decide the test if not already chosen\n",
    "    if self.testtype is None:\n",
    "        self.analysis_starter(alpha=alpha)\n",
    "\n",
    "    a = np.asarray(self.A, dtype=float)\n",
    "    b = np.asarray(self.B, dtype=float)\n",
    "\n",
    "    # Drop NaNs just in case\n",
    "    a = a[~np.isnan(a)]\n",
    "    b = b[~np.isnan(b)]\n",
    "\n",
    "    test = self.testtype\n",
    "    res = {\"test_used\": test, \"alpha\": alpha, \"n_a\": len(a), \"n_b\": len(b)}\n",
    "\n",
    "    # Map test types to actual scipy functions\n",
    "    if test == \"independent_ttest_equalvar\":\n",
    "        stat, p = stats.ttest_ind(a, b, equal_var=True)\n",
    "        eff = self._cohen_d_independent(a, b, equal_var=True)\n",
    "    elif test == \"independent_ttest_welch\":\n",
    "        stat, p = stats.ttest_ind(a, b, equal_var=False)\n",
    "        eff = self._cohen_d_independent(a, b, equal_var=False)\n",
    "    elif test == \"paired_ttest\":\n",
    "        stat, p = stats.ttest_rel(a, b)\n",
    "        eff = self._cohen_d_paired(a, b)\n",
    "    elif test == \"mannwhitneyu\":\n",
    "        # Use two-sided by default\n",
    "        stat, p = stats.mannwhitneyu(a, b, alternative=\"two-sided\")\n",
    "        eff = self._rank_biserial_from_mwu(a, b, stat)\n",
    "    elif test == \"wilcoxon\":\n",
    "        # Wilcoxon signed-rank for paired non-normal\n",
    "        stat, p = stats.wilcoxon(a, b, zero_method=\"wilcox\", correction=False, alternative=\"two-sided\", mode=\"auto\")\n",
    "        eff = self._rank_biserial_from_wilcoxon(a, b, stat)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown test type: {test}\")\n",
    "\n",
    "    res.update({\n",
    "        \"statistic\": float(stat),\n",
    "        \"p_value\": float(p),\n",
    "        \"significant\": bool(p < alpha),\n",
    "        \"effect_size\": float(eff) if eff is not None else None\n",
    "    })\n",
    "    self.result = res\n",
    "    return res\n",
    "\n",
    "\n",
    "# Effect size helpers (private)\n",
    "\n",
    "def _cohen_d_independent(self, a: np.ndarray, b: np.ndarray, equal_var: bool = True) -> float:\n",
    "    \"\"\"\n",
    "    Cohen's d for independent groups.\n",
    "    If equal_var=False (Welch), use the weighted pooled SD formula (still widely used).\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    na, nb = len(a), len(b)\n",
    "    ma, mb = np.nanmean(a), np.nanmean(b)\n",
    "    sa, sb = np.nanstd(a, ddof=1), np.nanstd(b, ddof=1)\n",
    "\n",
    "    if equal_var:\n",
    "        # Pooled standard deviation\n",
    "        sp2 = ((na - 1) * sa**2 + (nb - 1) * sb**2) / (na + nb - 2)\n",
    "        sp = np.sqrt(sp2)\n",
    "        if sp == 0: \n",
    "            return 0.0\n",
    "        return (ma - mb) / sp\n",
    "    else:\n",
    "        # Glass delta variants exist; here use Hedges' g approximation with pooled via weights\n",
    "        # (common pragmatic choice when equal variance is not assumed)\n",
    "        wp = ((sa**2) / na + (sb**2) / nb)\n",
    "        sp = np.sqrt(((sa**2 + sb**2) / 2.0))\n",
    "        if sp == 0:\n",
    "            return 0.0\n",
    "        d = (ma - mb) / sp\n",
    "        # Optional small-sample correction (Hedges' g)\n",
    "        df = ( (sa**2/na + sb**2/nb)**2 ) / ( ((sa**2/na)**2)/(na-1) + ((sb**2/nb)**2)/(nb-1) )\n",
    "        J = 1 - (3/(4*df - 1)) if df > 1 else 1.0\n",
    "        return float(J * d)\n",
    "\n",
    "\n",
    "def _cohen_d_paired(self, a: np.ndarray, b: np.ndarray) -> float:\n",
    "    \"\"\"\n",
    "    Cohen's d for paired samples: mean(diff) / sd(diff).\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    d = a - b\n",
    "    md = np.nanmean(d)\n",
    "    sd = np.nanstd(d, ddof=1)\n",
    "    if sd == 0:\n",
    "        return 0.0\n",
    "    return md / sd\n",
    "\n",
    "\n",
    "def _rank_biserial_from_mwu(self, a: np.ndarray, b: np.ndarray, u_stat: float) -> float:\n",
    "    \"\"\"\n",
    "    Rank-biserial correlation derived from Mann–Whitney U:\n",
    "    r_rb = 1 - (2U) / (n_a * n_b)\n",
    "    Range: [-1, 1]\n",
    "    \"\"\"\n",
    "    na, nb = len(a), len(b)\n",
    "    if na == 0 or nb == 0:\n",
    "        return 0.0\n",
    "    return 1.0 - (2.0 * u_stat) / (na * nb)\n",
    "\n",
    "\n",
    "def _rank_biserial_from_wilcoxon(self, a: np.ndarray, b: np.ndarray, w_stat: float) -> float:\n",
    "    \"\"\"\n",
    "    Approximate rank-biserial for Wilcoxon signed-rank:\n",
    "    r_rb = (W_pos - W_neg) / (W_pos + W_neg)\n",
    "    We can approximate via: r_rb = 1 - (2 * W_neg) / (W_pos + W_neg)\n",
    "    SciPy returns the test statistic W = sum of signed ranks positive? Historically it's min(W+, W-).\n",
    "    Here we compute directly from diffs for clarity.\n",
    "    \"\"\"\n",
    "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
    "    d = a - b\n",
    "    # Remove zeros (Wilcoxon ignores zeros)\n",
    "    d = d[d != 0]\n",
    "    if d.size == 0:\n",
    "        return 0.0\n",
    "\n",
    "    # Compute ranks of absolute differences\n",
    "    ranks = stats.rankdata(np.abs(d))\n",
    "    W_pos = np.sum(ranks[d > 0])\n",
    "    W_neg = np.sum(ranks[d < 0])\n",
    "    denom = (W_pos + W_neg)\n",
    "    if denom == 0:\n",
    "        return 0.0\n",
    "    return (W_pos - W_neg) / denom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eec8d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: interpretation & quick report\n",
    "    def _basic_group_stats(self) -> dict:\n",
    "        \"\"\"\n",
    "        Return simple descriptive stats for A and B (after dropping NaNs).\n",
    "        \"\"\"\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
    "        a = np.asarray(self.A, dtype=float); a = a[~np.isnan(a)]\n",
    "        b = np.asarray(self.B, dtype=float); b = b[~np.isnan(b)]\n",
    "        stats_a = {\n",
    "            \"n\": int(len(a)),\n",
    "            \"mean\": float(np.nanmean(a)) if len(a) else np.nan,\n",
    "            \"std\": float(np.nanstd(a, ddof=1)) if len(a) > 1 else np.nan,\n",
    "            \"median\": float(np.nanmedian(a)) if len(a) else np.nan,\n",
    "        }\n",
    "        stats_b = {\n",
    "            \"n\": int(len(b)),\n",
    "            \"mean\": float(np.nanmean(b)) if len(b) else np.nan,\n",
    "            \"std\": float(np.nanstd(b, ddof=1)) if len(b) > 1 else np.nan,\n",
    "            \"median\": float(np.nanmedian(b)) if len(b) else np.nan,\n",
    "        }\n",
    "        return {\"A\": stats_a, \"B\": stats_b}\n",
    "\n",
    "    def _effect_size_label(self, value: float) -> str:\n",
    "        \"\"\"\n",
    "        Heuristic label for effect size magnitude.\n",
    "        Uses Cohen's d style thresholds (also reasonable for rank-biserial abs values).\n",
    "        \"\"\"\n",
    "        if value is None or np.isnan(value):\n",
    "            return \"unknown\"\n",
    "        v = abs(value)\n",
    "        if v >= 0.8: return \"large\"\n",
    "        if v >= 0.5: return \"medium\"\n",
    "        if v >= 0.2: return \"small\"\n",
    "        return \"very small\"\n",
    "\n",
    "    def interpret(self, alpha: float = 0.05, verbose: bool = True):\n",
    "        \"\"\"\n",
    "        Human-friendly interpretation of the latest test result.\n",
    "        If no result is present, runs the test first.\n",
    "        Prints a short narrative if verbose=True.\n",
    "        Returns the result dict augmented with group stats.\n",
    "        \"\"\"\n",
    "        if self.result is None:\n",
    "            # Ensure test type is selected and run the test\n",
    "            self.run_test(alpha=alpha)\n",
    "\n",
    "        res = dict(self.result)  # copy\n",
    "        groups = self._basic_group_stats()\n",
    "        res[\"group_stats\"] = groups\n",
    "\n",
    "        # Build a short message\n",
    "        test_name = res.get(\"test_used\", \"unknown test\")\n",
    "        stat = res.get(\"statistic\", np.nan)\n",
    "        p = res.get(\"p_value\", np.nan)\n",
    "        eff = res.get(\"effect_size\", None)\n",
    "        sig = res.get(\"significant\", False)\n",
    "\n",
    "        # Pick mean or median difference depending on parametric vs non-parametric\n",
    "        a_mean, b_mean = groups[\"A\"][\"mean\"], groups[\"B\"][\"mean\"]\n",
    "        a_med, b_med = groups[\"A\"][\"median\"], groups[\"B\"][\"median\"]\n",
    "        if isinstance(test_name, str) and any(k in test_name for k in [\"mannwhitney\", \"wilcoxon\"]):\n",
    "            diff = float(a_med - b_med)\n",
    "            diff_label = \"median(A) - median(B)\"\n",
    "        else:\n",
    "            diff = float(a_mean - b_mean)\n",
    "            diff_label = \"mean(A) - mean(B)\"\n",
    "\n",
    "        res[\"difference\"] = {\"name\": diff_label, \"value\": diff}\n",
    "        res[\"effect_label\"] = self._effect_size_label(eff)\n",
    "\n",
    "        if verbose:\n",
    "            print(\"=\" * 60)\n",
    "            print(f\"Test: {test_name} | alpha={alpha}\")\n",
    "            print(f\"Statistic={stat:.4f} | p-value={p:.4g} | significant={sig}\")\n",
    "            if eff is not None:\n",
    "                print(f\"Effect size={eff:.3f} ({res['effect_label']})\")\n",
    "            print(f\"{diff_label} = {diff:.4f}\")\n",
    "            print(f\"A: n={groups['A']['n']}, mean={groups['A']['mean']:.4f}, median={groups['A']['median']:.4f}, std={groups['A']['std']}\")\n",
    "            print(f\"B: n={groups['B']['n']}, mean={groups['B']['mean']:.4f}, median={groups['B']['median']:.4f}, std={groups['B']['std']}\")\n",
    "            print(\"=\" * 60)\n",
    "\n",
    "        return res\n",
    "\n",
    "    def quick_report(self, alpha: float = 0.05) -> dict:\n",
    "        \"\"\"\n",
    "        One-call convenience: decide test (if needed), run it, and return a compact dict.\n",
    "        Does not print anything.\n",
    "        \"\"\"\n",
    "        if self.result is None or self.testtype is None:\n",
    "            self.analysis_starter(alpha=alpha)\n",
    "            self.run_test(alpha=alpha)\n",
    "\n",
    "        groups = self._basic_group_stats()\n",
    "        out = dict(self.result)\n",
    "        out[\"group_stats\"] = groups\n",
    "\n",
    "        # Also include a concise difference metric\n",
    "        test_name = out.get(\"test_used\", \"\")\n",
    "        if isinstance(test_name, str) and any(k in test_name for k in [\"mannwhitney\", \"wilcoxon\"]):\n",
    "            diff_name = \"median(A)-median(B)\"\n",
    "            diff_val = float(groups[\"A\"][\"median\"] - groups[\"B\"][\"median\"])\n",
    "        else:\n",
    "            diff_name = \"mean(A)-mean(B)\"\n",
    "            diff_val = float(groups[\"A\"][\"mean\"] - groups[\"B\"][\"mean\"])\n",
    "        out[\"difference\"] = {\"name\": diff_name, \"value\": diff_val}\n",
    "        out[\"effect_label\"] = self._effect_size_label(out.get(\"effect_size\", None))\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaff6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #Part 3A: DataFrame management & safe access\n",
    "\n",
    "    def set_df(self, df: pd.DataFrame):\n",
    "        \"\"\"\n",
    "        Attach a working DataFrame to the Analyzer instance.\n",
    "        This DF will be used by univariate/bivariate helper methods.\n",
    "        \"\"\"\n",
    "        if not isinstance(df, pd.DataFrame):\n",
    "            raise TypeError(\"set_df expects a pandas DataFrame.\")\n",
    "        # you can set it even if __init__ didn't define it\n",
    "        self.df = df.copy()\n",
    "        return self\n",
    "\n",
    "    def _ensure_df(self):\n",
    "        \"\"\"\n",
    "        Ensure that a DataFrame has been set via set_df().\n",
    "        \"\"\"\n",
    "        if not hasattr(self, \"df\") or self.df is None:\n",
    "            raise AttributeError(\"No DataFrame set. Call set_df(df) first.\")\n",
    "\n",
    "    def _get_series(self, col: str, dropna: bool = True) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Return a single column as a clean Series from self.df.\n",
    "        - Ensures the column exists\n",
    "        - Optionally drops NaNs\n",
    "        \"\"\"\n",
    "        self._ensure_df()\n",
    "        if col not in self.df.columns:\n",
    "            raise KeyError(f\"Column '{col}' not found in the DataFrame.\")\n",
    "        s = self.df[col]\n",
    "        if dropna:\n",
    "            s = s.dropna()\n",
    "        return s\n",
    "\n",
    "    def _is_categorical(self, s: pd.Series) -> bool:\n",
    "        \"\"\"\n",
    "        Decide if a Series should be treated as categorical for plotting/testing.\n",
    "        \"\"\"\n",
    "        return (str(s.dtype) in [\"object\", \"category\"]) or (s.nunique(dropna=True) <= max(12, int(0.05*len(s))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e90b384",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  Part 3B: Univariate descriptions (use self.df)\n",
    "\n",
    "    def describe_numeric(self, col: str, bins: int = 10, kde: bool = True, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Univariate summary for a numeric column in self.df:\n",
    "        - descriptive stats, missing, Shapiro p-value, skew, IQR outliers\n",
    "        - plots: histogram(+KDE), boxplot, PDF/CDF\n",
    "        Returns a dict with the computed statistics.\n",
    "        \"\"\"\n",
    "        s = self._get_series(col, dropna=True).astype(float)\n",
    "\n",
    "        # Basic stats\n",
    "        desc = s.describe()\n",
    "        shapiro_p = stats.shapiro(s).pvalue if len(s) >= 3 else np.nan\n",
    "        is_normal = (shapiro_p > alpha) if not np.isnan(shapiro_p) else False\n",
    "        skew_val = stats.skew(s)\n",
    "        q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
    "        iqr = q3 - q1\n",
    "        lower_b, upper_b = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        n_low = int((s < lower_b).sum()); n_high = int((s > upper_b).sum())\n",
    "        n_out = n_low + n_high\n",
    "        pct_out = 100.0 * n_out / len(s) if len(s) else 0.0\n",
    "        missing = int(self.df[col].isna().sum())\n",
    "\n",
    "        # --- plots ---\n",
    "        fig, ax = plt.subplots(2, 2, figsize=(16, 7), gridspec_kw={'height_ratios': (.85, .15)})\n",
    "\n",
    "        # Histogram (+ KDE)\n",
    "        sns.histplot(s, kde=kde, ax=ax[0, 0], color='#55A868')\n",
    "        ax[0, 0].set_title(f'Histogram of {col}')\n",
    "        ax[0, 0].set_xlabel('')\n",
    "        ax[0, 0].set_ylabel('Count')\n",
    "\n",
    "        # Boxplot\n",
    "        sns.boxplot(x=s, ax=ax[1, 0], color=\"#5583A8\", orient='h')\n",
    "        label_text = f\"Lower outliers: {n_low}\\nUpper outliers: {n_high}\\nTotal: {n_out} ({pct_out:.1f}%)\"\n",
    "        patch = mpatches.Patch(color='skyblue', label=label_text)\n",
    "        ax[1, 0].legend(handles=[patch], fontsize=10, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
    "\n",
    "        # PDF/CDF\n",
    "        counts, bin_edges = np.histogram(s, bins=bins, density=True)\n",
    "        pdf = counts / counts.sum() if counts.sum() != 0 else counts\n",
    "        cdf = np.cumsum(pdf)\n",
    "        ax[1, 1] = plt.subplot(122)\n",
    "        plt.plot(bin_edges[1:], pdf, label='PDF')\n",
    "        plt.plot(bin_edges[1:], cdf, label='CDF')\n",
    "        plt.legend()\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        # Tidy\n",
    "        ax[0, 0].set_xticklabels([])\n",
    "        ax[1, 0].set_yticklabels([])\n",
    "        fig.suptitle(col, fontsize=18)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        info = {\n",
    "            \"count\": int(desc.get(\"count\", 0)),\n",
    "            \"mean\": float(desc.get(\"mean\", np.nan)),\n",
    "            \"std\": float(desc.get(\"std\", np.nan)),\n",
    "            \"min\": float(desc.get(\"min\", np.nan)),\n",
    "            \"25%\": float(desc.get(\"25%\", np.nan)),\n",
    "            \"50%\": float(desc.get(\"50%\", np.nan)),\n",
    "            \"75%\": float(desc.get(\"75%\", np.nan)),\n",
    "            \"max\": float(desc.get(\"max\", np.nan)),\n",
    "            \"missing\": missing,\n",
    "            \"shapiro_p\": float(shapiro_p) if not np.isnan(shapiro_p) else None,\n",
    "            \"normal\": bool(is_normal),\n",
    "            \"skew\": float(skew_val),\n",
    "            \"iqr_lower_bound\": float(lower_b),\n",
    "            \"iqr_upper_bound\": float(upper_b),\n",
    "            \"outliers_lower\": n_low,\n",
    "            \"outliers_upper\": n_high,\n",
    "            \"outliers_total_pct\": round(pct_out, 2)\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    def describe_categorical(self, col: str, top_n: int = None):\n",
    "        \"\"\"\n",
    "        Univariate summary for a categorical column in self.df:\n",
    "        - frequency table with percentages\n",
    "        - bar plot of counts (optionally top_n categories)\n",
    "        Returns a dict with counts and percents.\n",
    "        \"\"\"\n",
    "        s = self._get_series(col, dropna=False)  # keep NaN to report missing\n",
    "        counts = s.value_counts(dropna=True)\n",
    "        if top_n is not None and top_n > 0:\n",
    "            counts = counts.head(top_n)\n",
    "        total_non_na = counts.sum()\n",
    "        perc = (counts / total_non_na * 100.0).round(2) if total_non_na else counts\n",
    "\n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 5))\n",
    "        sns.barplot(x=counts.index.astype(str), y=counts.values, color=\"#55A868\")\n",
    "        plt.title(f'Counts of {col}')\n",
    "        plt.ylabel('Count')\n",
    "        plt.xlabel(col)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        info = {\n",
    "            \"missing\": int(s.isna().sum()),\n",
    "            \"unique\": int(s.nunique(dropna=True)),\n",
    "            \"counts\": counts.to_dict(),\n",
    "            \"percents\": perc.to_dict()\n",
    "        }\n",
    "        return info\n",
    "\n",
    "    def bin_numeric(self, col: str, bins, labels=None, new_col: str = None,\n",
    "                    right: bool = True, include_lowest: bool = True):\n",
    "        \"\"\"\n",
    "        Create categorical bins from a numeric column using pd.cut.\n",
    "        - bins: list/array of bin edges\n",
    "        - labels: optional labels for each bin\n",
    "        - new_col: optional name for the new column; defaults to f\"{col}_binned\"\n",
    "        Returns the created Series.\n",
    "        \"\"\"\n",
    "        self._ensure_df()\n",
    "        if new_col is None:\n",
    "            new_col = f\"{col}_binned\"\n",
    "        s = self._get_series(col, dropna=False)\n",
    "        binned = pd.cut(s, bins=bins, labels=labels, right=right, include_lowest=include_lowest)\n",
    "        self.df[new_col] = binned\n",
    "        return self.df[new_col]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f1a4996",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  Part 3C: Bivariate analyses (use self.df)\n",
    "\n",
    "    def rel(self, feature: str, target: str, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Dispatcher: choose the appropriate relationship method based on dtypes.\n",
    "        \"\"\"\n",
    "        x = self._get_series(feature, dropna=False)\n",
    "        y = self._get_series(target, dropna=False)\n",
    "\n",
    "        # Decide categorical vs numeric\n",
    "        x_is_cat = self._is_categorical(x)\n",
    "        y_is_cat = self._is_categorical(y)\n",
    "\n",
    "        if not x_is_cat and not y_is_cat:\n",
    "            return self.rel_num_num(feature, target, alpha=alpha)\n",
    "        elif x_is_cat and y_is_cat:\n",
    "            return self.rel_cat_cat(feature, target, alpha=alpha)\n",
    "        else:\n",
    "            # Ensure (cat, num) order\n",
    "            if x_is_cat and not y_is_cat:\n",
    "                return self.rel_cat_num(cat_col=feature, num_col=target, alpha=alpha)\n",
    "            else:\n",
    "                return self.rel_cat_num(cat_col=target, num_col=feature, alpha=alpha)\n",
    "\n",
    "    def rel_num_num(self, col1: str, col2: str, alpha: float = 0.05, gridsize: int = 20):\n",
    "        \"\"\"\n",
    "        Numeric vs Numeric:\n",
    "        - Normality via Shapiro (if len>=3)\n",
    "        - Pearson if both normal else Spearman\n",
    "        - Plots: scatter + hexbin\n",
    "        Returns dict with test info.\n",
    "        \"\"\"\n",
    "        s1 = self._get_series(col1, dropna=True).astype(float)\n",
    "        s2 = self._get_series(col2, dropna=True).astype(float)\n",
    "\n",
    "        # align indices (just in case lengths differ after dropna)\n",
    "        df_pair = pd.DataFrame({col1: s1, col2: s2}).dropna()\n",
    "        x = df_pair[col1].values\n",
    "        y = df_pair[col2].values\n",
    "\n",
    "        sh1 = stats.shapiro(x).pvalue if len(x) >= 3 else np.nan\n",
    "        sh2 = stats.shapiro(y).pvalue if len(y) >= 3 else np.nan\n",
    "        both_normal = (sh1 > alpha if not np.isnan(sh1) else False) and (sh2 > alpha if not np.isnan(sh2) else False)\n",
    "\n",
    "        if both_normal:\n",
    "            test_used = \"Pearson\"\n",
    "            r, p = stats.pearsonr(x, y)\n",
    "        else:\n",
    "            test_used = \"Spearman\"\n",
    "            r, p = stats.spearmanr(x, y)\n",
    "\n",
    "        r2 = r ** 2\n",
    "        strength = \"strong\" if abs(r) > 0.7 else (\"moderate\" if abs(r) > 0.3 else \"weak\")\n",
    "        direction = \"positive\" if r > 0 else \"negative\"\n",
    "\n",
    "        # --- plots ---\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "        fig.suptitle(f'{col1} vs {col2} - {test_used} correlation', fontsize=14)\n",
    "\n",
    "        # Scatter\n",
    "        axes[0].scatter(x, y, s=50, alpha=0.7, color='blue', edgecolors='black', linewidth=0.5)\n",
    "        axes[0].set_title('Scatter')\n",
    "        axes[0].set_xlabel(col1); axes[0].set_ylabel(col2); axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "        # Hexbin\n",
    "        hb = axes[1].hexbin(x, y, gridsize=gridsize, cmap='Blues', mincnt=1)\n",
    "        axes[1].set_title('Hexbin'); axes[1].set_xlabel(col1); axes[1].set_ylabel(col2)\n",
    "        cb = fig.colorbar(hb, ax=axes[1]); cb.set_label('count')\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        return {\n",
    "            \"test_used\": test_used,\n",
    "            \"shapiro_p_col1\": float(sh1) if not np.isnan(sh1) else None,\n",
    "            \"shapiro_p_col2\": float(sh2) if not np.isnan(sh2) else None,\n",
    "            \"correlation\": float(r),\n",
    "            \"r_squared\": float(r2),\n",
    "            \"p_value\": float(p),\n",
    "            \"significant\": bool(p < alpha),\n",
    "            \"relationship_strength\": strength,\n",
    "            \"relationship_direction\": direction,\n",
    "            \"n\": int(len(x))\n",
    "        }\n",
    "\n",
    "    def rel_cat_num(self, cat_col: str, num_col: str, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Categorical vs Numeric:\n",
    "        - Shapiro per group; if all normal -> ANOVA; else Welch's ANOVA (if available) else Kruskal-Wallis\n",
    "        - Plots: box, violin, mean bar, count of categories\n",
    "        Returns dict with test info + per-group normality.\n",
    "        \"\"\"\n",
    "        s_cat = self._get_series(cat_col, dropna=False)\n",
    "        s_num = self._get_series(num_col, dropna=True).astype(float)\n",
    "\n",
    "        # Align and drop NaNs in numeric and category simultaneously\n",
    "        df_local = pd.DataFrame({cat_col: s_cat, num_col: self.df[num_col]}).dropna()\n",
    "        # Re-extract arrays\n",
    "        groups = df_local.groupby(cat_col)[num_col].describe()\n",
    "        normality = {}\n",
    "        data_groups = []\n",
    "        all_normal = True\n",
    "\n",
    "        for name, grp in df_local.groupby(cat_col):\n",
    "            vals = grp[num_col].values\n",
    "            if len(vals) >= 3:\n",
    "                p_sh = stats.shapiro(vals).pvalue\n",
    "                is_norm = p_sh > alpha\n",
    "                normality[name] = {\"shapiro_p\": float(p_sh), \"is_normal\": bool(is_norm), \"n\": int(len(vals))}\n",
    "                if not is_norm: all_normal = False\n",
    "            else:\n",
    "                normality[name] = {\"shapiro_p\": None, \"is_normal\": False, \"n\": int(len(vals))}\n",
    "                all_normal = False\n",
    "            data_groups.append(vals)\n",
    "\n",
    "        # Choose test\n",
    "        test_used = None\n",
    "        stat, p_val = np.nan, np.nan\n",
    "        if len(data_groups) >= 2:\n",
    "            if all_normal:\n",
    "                test_used = \"ANOVA\"\n",
    "                stat, p_val = stats.f_oneway(*data_groups)\n",
    "            else:\n",
    "                # Try Alexander-Govern (Welch's ANOVA in SciPy name) if available\n",
    "                try:\n",
    "                    from scipy.stats import alexandergovern\n",
    "                    result = alexandergovern(*data_groups)\n",
    "                    stat, p_val = float(result.statistic), float(result.pvalue)\n",
    "                    test_used = \"Welch_ANOVA\"\n",
    "                except Exception:\n",
    "                    stat, p_val = stats.kruskal(*data_groups)\n",
    "                    test_used = \"Kruskal_Wallis\"\n",
    "        else:\n",
    "            test_used = \"insufficient_groups\"\n",
    "\n",
    "        # --- plots ---\n",
    "        fig = plt.figure(figsize=(14, 10))\n",
    "\n",
    "        ax1 = plt.subplot(221)\n",
    "        sns.boxplot(data=df_local, x=cat_col, y=num_col, ax=ax1)\n",
    "        ax1.set_title(f'{num_col} by {cat_col}'); plt.xticks(rotation=45)\n",
    "\n",
    "        ax2 = plt.subplot(222)\n",
    "        sns.violinplot(data=df_local, x=cat_col, y=num_col, ax=ax2)\n",
    "        ax2.set_title(f'{num_col} distribution by {cat_col}'); plt.xticks(rotation=45)\n",
    "\n",
    "        ax3 = plt.subplot(223)\n",
    "        means = df_local.groupby(cat_col)[num_col].mean().sort_values(ascending=False)\n",
    "        sns.barplot(x=means.index, y=means.values, ax=ax3)\n",
    "        ax3.set_title(f'Mean {num_col} by {cat_col}'); plt.xticks(rotation=45)\n",
    "\n",
    "        ax4 = plt.subplot(224)\n",
    "        sns.countplot(data=df_local, x=cat_col, ax=ax4)\n",
    "        ax4.set_title(f'Count of {cat_col}'); plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        return {\n",
    "            \"test_used\": test_used,\n",
    "            \"all_groups_normal\": bool(all_normal),\n",
    "            \"statistic\": float(stat) if not np.isnan(stat) else None,\n",
    "            \"p_value\": float(p_val) if not np.isnan(p_val) else None,\n",
    "            \"significant\": (p_val < alpha) if not np.isnan(p_val) else None,\n",
    "            \"num_categories\": int(df_local[cat_col].nunique()),\n",
    "            \"total_observations\": int(len(df_local)),\n",
    "            \"group_stats\": groups.to_dict(),\n",
    "            \"normality\": normality\n",
    "        }\n",
    "\n",
    "    def rel_cat_cat(self, col1: str, col2: str, alpha: float = 0.05):\n",
    "        \"\"\"\n",
    "        Categorical vs Categorical:\n",
    "        - Contingency table + Chi-square test\n",
    "        - Effect size: Cramér's V\n",
    "        - Plots: heatmap (counts), stacked 100% bar, normalized heatmap, grouped counts\n",
    "        Returns dict with chi2, p, dof, Cramér's V.\n",
    "        \"\"\"\n",
    "        s1 = self._get_series(col1, dropna=False).astype(\"category\")\n",
    "        s2 = self._get_series(col2, dropna=False).astype(\"category\")\n",
    "\n",
    "        df_local = pd.DataFrame({col1: s1, col2: s2}).dropna()\n",
    "        table = pd.crosstab(df_local[col1], df_local[col2])\n",
    "        norm = pd.crosstab(df_local[col1], df_local[col2], normalize='index')\n",
    "        n = table.values.sum()\n",
    "\n",
    "        # Chi-square test\n",
    "        chi2, p_val, dof, expected = stats.chi2_contingency(table)\n",
    "\n",
    "        # Check expected counts rule-of-thumb\n",
    "        too_small = (expected < 5).sum()\n",
    "        expected_ok = (too_small / expected.size) <= 0.2  # no more than 20% < 5\n",
    "\n",
    "        # Cramér's V\n",
    "        k = min(table.shape) - 1\n",
    "        cramers_v = np.sqrt(chi2 / (n * max(k, 1)))\n",
    "\n",
    "        strength = \"strong\" if cramers_v > 0.3 else (\"moderate\" if cramers_v > 0.1 else \"weak\")\n",
    "\n",
    "        # --- plots ---\n",
    "        fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        sns.heatmap(table, annot=True, fmt='d', cbar=False, cmap='YlGnBu',\n",
    "                    ax=axs[0, 0], linecolor='lightgray', linewidths=0.7)\n",
    "        axs[0, 0].set_title(f'Contingency: {col1} vs {col2}')\n",
    "\n",
    "        norm.plot.bar(stacked=True, ax=axs[0, 1])\n",
    "        axs[0, 1].set_title('Stacked Bar (100%)')\n",
    "        axs[0, 1].legend(title=col2)\n",
    "        axs[0, 1].set_xlabel(col1); axs[0, 1].set_ylabel('Proportion')\n",
    "\n",
    "        sns.heatmap(norm, annot=True, fmt='.2%', cbar=False, cmap='YlGnBu',\n",
    "                    ax=axs[1, 0], linecolor='lightgray', linewidths=0.7)\n",
    "        axs[1, 0].set_title('Row-normalized (%)')\n",
    "\n",
    "        table.plot(kind='bar', ax=axs[1, 1])\n",
    "        axs[1, 1].set_title('Counts by group'); axs[1, 1].legend(title=col2)\n",
    "        plt.xticks(rotation=45)\n",
    "\n",
    "        plt.tight_layout(); plt.show()\n",
    "\n",
    "        return {\n",
    "            \"chi2_statistic\": float(chi2),\n",
    "            \"p_value\": float(p_val),\n",
    "            \"degrees_of_freedom\": int(dof),\n",
    "            \"cramers_v\": float(cramers_v),\n",
    "            \"association_strength\": strength,\n",
    "            \"significant_association\": bool(p_val < alpha),\n",
    "            \"expected_counts_ok\": bool(expected_ok),\n",
    "            \"contingency_table\": table.to_dict(),\n",
    "            \"normalized_row\": norm.to_dict(),\n",
    "            \"n\": int(n)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275dcc4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  Part 3D: KPI helpers & plots (use self.df)\n",
    "\n",
    "    def _get_grouped_data(self, group_cols, kpi: str = \"Conversion Rate\"):\n",
    "        \"\"\"\n",
    "        Internal helper: group self.df by group_cols and compute KPI.\n",
    "        Supported KPIs:\n",
    "          - \"Conversion Rate\" = conversions / clicks * 100\n",
    "          - \"CPC\"             = cost / clicks\n",
    "        Expected raw columns (if you want auto-compute):\n",
    "          - 'clicks', 'conversions', 'cost'\n",
    "        If KPI column already exists in self.df, grouping will aggregate by mean.\n",
    "        Returns a grouped DataFrame with columns: group_cols + [kpi]\n",
    "        \"\"\"\n",
    "        self._ensure_df()\n",
    "        if isinstance(group_cols, (str,)):\n",
    "            group_cols = [group_cols]\n",
    "\n",
    "        df = self.df.copy()\n",
    "\n",
    "        # If KPI column missing, try to compute from standard raw columns\n",
    "        if kpi not in df.columns:\n",
    "            # Auto-compute if raw columns exist\n",
    "            if kpi == \"Conversion Rate\":\n",
    "                if all(c in df.columns for c in [\"conversions\", \"clicks\"]):\n",
    "                    # Avoid division by zero\n",
    "                    df[\"Conversion Rate\"] = np.where(df[\"clicks\"] > 0,\n",
    "                                                     (df[\"conversions\"] / df[\"clicks\"]) * 100.0, np.nan)\n",
    "                else:\n",
    "                    raise KeyError(\"To compute 'Conversion Rate', columns 'conversions' and 'clicks' must exist.\")\n",
    "            elif kpi == \"CPC\":\n",
    "                if all(c in df.columns for c in [\"cost\", \"clicks\"]):\n",
    "                    df[\"CPC\"] = np.where(df[\"clicks\"] > 0, df[\"cost\"] / df[\"clicks\"], np.nan)\n",
    "                else:\n",
    "                    raise KeyError(\"To compute 'CPC', columns 'cost' and 'clicks' must exist.\")\n",
    "            else:\n",
    "                raise ValueError(\"KPI must be either 'Conversion Rate' or 'CPC'.\")\n",
    "\n",
    "        # Group and aggregate KPI by mean (typical for rates/cost per click after per-row calc)\n",
    "        grouped = df.groupby(group_cols, dropna=False, as_index=False)[kpi].mean()\n",
    "        return grouped\n",
    "\n",
    "    def kpi_heatmap(self, group1: str, group2: str, kpi: str = \"Conversion Rate\"):\n",
    "        \"\"\"\n",
    "        Draw a heatmap of KPI by two categorical dimensions.\n",
    "        \"\"\"\n",
    "        if kpi not in [\"Conversion Rate\", \"CPC\"]:\n",
    "            raise ValueError('KPI must be \"Conversion Rate\" or \"CPC\"')\n",
    "\n",
    "        tmp = self._get_grouped_data([group1, group2], kpi=kpi)\n",
    "\n",
    "        heatmap_data = tmp.pivot_table(index=group1, columns=group2, values=kpi)\n",
    "        heatmap_data = heatmap_data.fillna(0)\n",
    "\n",
    "        plt.figure(figsize=(18, 8))\n",
    "        sns.heatmap(\n",
    "            heatmap_data,\n",
    "            cmap='YlGnBu',\n",
    "            annot=True,\n",
    "            linewidths=0.7,\n",
    "            linecolor='lightgray'\n",
    "        )\n",
    "        plt.title(f'Heatmap of {kpi} by {group1} / {group2}', fontsize=18, weight='bold')\n",
    "        plt.ylabel(group1); plt.xlabel(group2)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return heatmap_data  # return matrix for possible reuse\n",
    "\n",
    "    def kpi_bar(self, group_col: str, kpi: str = \"Conversion Rate\"):\n",
    "        \"\"\"\n",
    "        Draw a barplot of KPI by a single categorical dimension and annotate bars.\n",
    "        \"\"\"\n",
    "        if kpi not in [\"Conversion Rate\", \"CPC\"]:\n",
    "            raise ValueError('KPI must be \"Conversion Rate\" or \"CPC\"')\n",
    "\n",
    "        tmp = self._get_grouped_data(group_col, kpi=kpi)\n",
    "\n",
    "        plt.figure(figsize=(12, 7))\n",
    "        ax = sns.barplot(data=tmp, x=group_col, y=kpi, palette='viridis')\n",
    "        ax.set_title(f'{kpi} by {group_col}')\n",
    "        ax.set_xlabel(group_col); ax.set_ylabel(kpi)\n",
    "        plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "        # Annotate each bar with a friendly label\n",
    "        for p in ax.patches:\n",
    "            val = p.get_height()\n",
    "            if np.isnan(val):\n",
    "                label = \"NA\"\n",
    "            else:\n",
    "                label = f'{val:.2f}%' if kpi == \"Conversion Rate\" else f'{val:.4f}$'\n",
    "            ax.annotate(label,\n",
    "                        (p.get_x() + p.get_width()/2., val if not np.isnan(val) else 0),\n",
    "                        ha='center', va='center', fontsize=11, color='black',\n",
    "                        xytext=(0, 10), textcoords='offset points')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        return tmp  # return grouped table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092d109f",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  Part 4: A/B visualizations (use self.A and self.B)\n",
    "\n",
    "    def visual_groups(self, bins: int = 20, kde: bool = True, title: str = None):\n",
    "        \"\"\"\n",
    "        Visual comparison of groups A and B:\n",
    "        - overlaid histograms (+optional KDE)\n",
    "        - side-by-side boxplots\n",
    "        - side-by-side violin plots\n",
    "        - Q–Q plots vs Normal\n",
    "        Requirements: self.A and self.B set via fit().\n",
    "        \"\"\"\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
    "\n",
    "        a = np.asarray(self.A, dtype=float)\n",
    "        b = np.asarray(self.B, dtype=float)\n",
    "        a = a[~np.isnan(a)]\n",
    "        b = b[~np.isnan(b)]\n",
    "\n",
    "        if len(a) == 0 or len(b) == 0:\n",
    "            raise ValueError(\"A and/or B contain no valid numeric values after NaN removal.\")\n",
    "\n",
    "        # --- Figure layout ---\n",
    "        fig = plt.figure(figsize=(16, 10))\n",
    "        if title is None:\n",
    "            title = \"A vs B — distribution & diagnostics\"\n",
    "        fig.suptitle(title, fontsize=16, y=0.98)\n",
    "\n",
    "        # 1) Overlaid histograms (+KDE)\n",
    "        ax1 = plt.subplot(221)\n",
    "        sns.histplot(a, bins=bins, kde=kde, stat='density', color=\"#4C78A8\", alpha=0.45, ax=ax1, label=\"A\")\n",
    "        sns.histplot(b, bins=bins, kde=kde, stat='density', color=\"#F58518\", alpha=0.45, ax=ax1, label=\"B\")\n",
    "        ax1.set_title(\"Overlaid histograms\")\n",
    "        ax1.set_xlabel(\"\"); ax1.set_ylabel(\"Density\")\n",
    "        ax1.legend()\n",
    "\n",
    "        # 2) Boxplots (side-by-side)\n",
    "        ax2 = plt.subplot(222)\n",
    "        sns.boxplot(data=[a, b], orient='v', ax=ax2, palette=[\"#4C78A8\", \"#F58518\"])\n",
    "        ax2.set_title(\"Boxplots (A, B)\")\n",
    "        ax2.set_xticklabels([\"A\", \"B\"])\n",
    "\n",
    "        # 3) Violin plots (side-by-side)\n",
    "        ax3 = plt.subplot(223)\n",
    "        # Build a compact DataFrame for seaborn violin\n",
    "        df_tmp = pd.DataFrame({\n",
    "            \"value\": np.r_[a, b],\n",
    "            \"group\": [\"A\"] * len(a) + [\"B\"] * len(b)\n",
    "        })\n",
    "        sns.violinplot(data=df_tmp, x=\"group\", y=\"value\", ax=ax3, palette=[\"#4C78A8\", \"#F58518\"])\n",
    "        ax3.set_title(\"Violin plots\")\n",
    "\n",
    "        # 4) Q–Q plots against Normal\n",
    "        ax4 = plt.subplot(224)\n",
    "        # For readability, draw QQ for both A and B sequentially\n",
    "        stats.probplot(a, dist=\"norm\", plot=ax4)\n",
    "        stats.probplot(b, dist=\"norm\", plot=ax4)\n",
    "        ax4.set_title(\"Q–Q plots vs Normal\")\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "        plt.show()\n",
    "\n",
    "    def visual_groups_pdf_cdf(self, bins: int = 20):\n",
    "        \"\"\"\n",
    "        Plot PDF/CDF curves for groups A and B in a single axes pair.\n",
    "        Good for comparing cumulative behavior (stochastic dominance).\n",
    "        \"\"\"\n",
    "        if self.A is None or self.B is None:\n",
    "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
    "        a = np.asarray(self.A, dtype=float); a = a[~np.isnan(a)]\n",
    "        b = np.asarray(self.B, dtype=float); b = b[~np.isnan(b)]\n",
    "\n",
    "        if len(a) == 0 or len(b) == 0:\n",
    "            raise ValueError(\"A and/or B contain no valid numeric values after NaN removal.\")\n",
    "\n",
    "        # Compute PDF/CDF for A\n",
    "        ca, ba = np.histogram(a, bins=bins, density=True)\n",
    "        pdf_a = ca / ca.sum() if ca.sum() != 0 else ca\n",
    "        cdf_a = np.cumsum(pdf_a)\n",
    "\n",
    "        # Compute PDF/CDF for B\n",
    "        cb, bb = np.histogram(b, bins=bins, density=True)\n",
    "        pdf_b = cb / cb.sum() if cb.sum() != 0 else cb\n",
    "        cdf_b = np.cumsum(pdf_b)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "        # PDF\n",
    "        axes[0].plot(ba[1:], pdf_a, label=\"A - PDF\")\n",
    "        axes[0].plot(bb[1:], pdf_b, label=\"B - PDF\")\n",
    "        axes[0].set_title(\"PDF comparison\"); axes[0].legend()\n",
    "\n",
    "        # CDF\n",
    "        axes[1].plot(ba[1:], cdf_a, label=\"A - CDF\")\n",
    "        axes[1].plot(bb[1:], cdf_b, label=\"B - CDF\")\n",
    "        axes[1].set_title(\"CDF comparison\"); axes[1].legend()\n",
    "\n",
    "        for ax in axes:\n",
    "            ax.grid(True, alpha=0.25)\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d15eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "    #  Part 5: final wiring & cleanup\n",
    "\n",
    "    def visual_description(self, bins: int = 20, kde: bool = True, title: str = None):\n",
    "        \"\"\"\n",
    "        Quick visual overview for groups A and B.\n",
    "        Shows overlaid histograms, box/violin, and QQ plots.\n",
    "        \"\"\"\n",
    "        self.visual_groups(bins=bins, kde=kde, title=title)\n",
    "\n",
    "    # (optional) keep legacy name but warn; not used anymore\n",
    "    def __initial_test(self, test):\n",
    "        \"\"\"\n",
    "        DEPRECATED: kept only for backward compatibility.\n",
    "        Use analysis_starter() + run_test() instead.\n",
    "        \"\"\"\n",
    "        raise DeprecationWarning(\n",
    "            \"Use analysis_starter() to choose the test, then run_test() to execute it.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b33d327",
   "metadata": {},
   "source": [
    "۱) ساخت شیء از کلاس:\n",
    "`an = Analyzer()`\n",
    "\n",
    "۲) اتصال دیتافریم پروژه برای تحلیل‌های تک‌متغیره/دومتغیره/KPI:\n",
    "`an.set_df(df)`\n",
    "\n",
    "۳) توصیف تک‌متغیره:\n",
    "\n",
    "* عددی: `an.describe_numeric('height_cm', bins=15, kde=True)` → خلاصه آماری، شاپیرو، آوتلایرها، هیستو/باکس/PDF-CDF\n",
    "* دسته‌ای: `an.describe_categorical('position')` → جدول فراوانی/درصد + نمودار ستونی\n",
    "* بَندی عددی:\n",
    "  `an.bin_numeric('age', bins=[0,20,25,30,35,99], labels=['<20','20-25','25-30','30-35','35+'])`\n",
    "\n",
    "۴) توصیف دومتغیره (تشخیص خودکار نوع رابطه):\n",
    "\n",
    "* دسته‌ای×دسته‌ای: `an.rel('position', 'is_champion')` → کای‌دو + کرامر V\n",
    "* دسته‌ای×عددی: `an.rel('position', 'WS')` → ANOVA/Welch/Kruskal + نمودارها\n",
    "* عددی×عددی: `an.rel('WS', 'VORP')` → Pearson/Spearman + Scatter/Hexbin\n",
    "* خروجی هر مورد: دیکشنری نتایج آماری.\n",
    "\n",
    "۵) KPI (در صورت وجود ستون‌های مرتبط: `clicks`, `conversions`, `cost` یا KPI از پیش‌محاسبه‌شده):\n",
    "\n",
    "* هیت‌مپ: `an.kpi_heatmap(group1='campaign', group2='device', kpi='Conversion Rate')`\n",
    "* نمودار ستونی: `an.kpi_bar(group_col='campaign', kpi='CPC')`\n",
    "\n",
    "۶) مقایسهٔ دو گروه A/B (مثال: Top-15 در برابر روستر قهرمان روی «تجربه»):\n",
    "\n",
    "* آماده‌سازی آرایه‌ها: `X = df_top15['experience'].values` ، `Y = df_champion['experience'].values`\n",
    "* تنظیم گروه‌ها: `an.fit(X, Y, paired=False)`  (برای دادهٔ جفتی: `paired=True`)\n",
    "* انتخاب تست: `an.analysis_starter(alpha=0.05)`\n",
    "* اجرای تست: `an.run_test(alpha=0.05)`\n",
    "* تفسیر و خلاصه: `an.interpret(alpha=0.05, verbose=True)`\n",
    "* گراف‌های تشخیصی A/B:\n",
    "\n",
    "  * توزیع/باکس/ویولن/QQ: `an.visual_groups(bins=20, kde=True, title='Top-15 vs Champion — Experience')`\n",
    "  * PDF/CDF: `an.visual_groups_pdf_cdf(bins=20)`\n",
    "\n",
    "۷) نکات:\n",
    "\n",
    "* بررسی نام/نوع ستون‌ها و مدیریت `NaN`.\n",
    "* در تست‌های ناپارامتریک (Mann–Whitney/Wilcoxon) تفاوت **میانه** تفسیر شود؛ در پارامتریک‌ها تفاوت **میانگین**.\n",
    "* گزارش همزمان **p-value** و **اندازه‌اثر** (Cohen’s d یا rank-biserial).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9471f16d",
   "metadata": {},
   "source": [
    "1. Create the class instance:\n",
    "   `an = Analyzer()`\n",
    "\n",
    "2. Attach your project DataFrame for univariate/bivariate/KPI work:\n",
    "   `an.set_df(df)`\n",
    "\n",
    "3. Univariate description:\n",
    "\n",
    "* Numeric: `an.describe_numeric('height_cm', bins=15, kde=True)` → summary stats, Shapiro, outliers, hist/box/PDF-CDF\n",
    "* Categorical: `an.describe_categorical('position')` → frequency/percent table + bar chart\n",
    "* Numeric binning:\n",
    "  `an.bin_numeric('age', bins=[0,20,25,30,35,99], labels=['<20','20-25','25-30','30-35','35+'])`\n",
    "\n",
    "4. Bivariate description (auto type detection):\n",
    "\n",
    "* Categorical × Categorical: `an.rel('position', 'is_champion')` → Chi-square + Cramér’s V\n",
    "* Categorical × Numeric: `an.rel('position', 'WS')` → ANOVA / Welch / Kruskal + plots\n",
    "* Numeric × Numeric: `an.rel('WS', 'VORP')` → Pearson / Spearman + Scatter/Hexbin\n",
    "* Output of each call: a result **dict** with test stats.\n",
    "\n",
    "5. KPI (if relevant columns exist: `clicks`, `conversions`, `cost`, or precomputed KPI):\n",
    "\n",
    "* Heatmap: `an.kpi_heatmap(group1='campaign', group2='device', kpi='Conversion Rate')`\n",
    "* Bar plot: `an.kpi_bar(group_col='campaign', kpi='CPC')`\n",
    "\n",
    "6. A/B comparison (example: Top-15 vs Champion roster on “experience”):\n",
    "\n",
    "* Prepare arrays: `X = df_top15['experience'].values`, `Y = df_champion['experience'].values`\n",
    "* Set groups: `an.fit(X, Y, paired=False)`  (use `paired=True` for paired data)\n",
    "* Choose test: `an.analysis_starter(alpha=0.05)`\n",
    "* Run test: `an.run_test(alpha=0.05)`\n",
    "* Interpret + summary: `an.interpret(alpha=0.05, verbose=True)`\n",
    "* A/B diagnostics:\n",
    "\n",
    "  * Distributions/box/violin/QQ: `an.visual_groups(bins=20, kde=True, title='Top-15 vs Champion — Experience')`\n",
    "  * PDF/CDF comparison: `an.visual_groups_pdf_cdf(bins=20)`\n",
    "\n",
    "7. Notes:\n",
    "\n",
    "* Verify column names/types and handle `NaN`.\n",
    "* For nonparametric tests (Mann–Whitney/Wilcoxon) interpret **median** differences; for parametric tests interpret **mean** differences.\n",
    "* Report **p-value** alongside **effect size** (Cohen’s d or rank-biserial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67313a55",
   "metadata": {
    "id": "67313a55"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pd' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m x \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m23\u001b[39m, \u001b[38;5;241m24\u001b[39m, \u001b[38;5;241m25\u001b[39m]\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mage\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m22\u001b[39m, \u001b[38;5;241m25\u001b[39m, \u001b[38;5;241m30\u001b[39m],\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscore\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;241m70\u001b[39m, \u001b[38;5;241m80\u001b[39m, \u001b[38;5;241m90\u001b[39m, \u001b[38;5;241m85\u001b[39m]\n\u001b[0;32m      5\u001b[0m })\n\u001b[0;32m      7\u001b[0m stat_ob \u001b[38;5;241m=\u001b[39m Analyzer()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pd' is not defined"
     ]
    }
   ],
   "source": [
    "x = [22, 23, 24, 25]\n",
    "df = df = pd.DataFrame({\n",
    "    \"age\": [20, 22, 25, 30],\n",
    "    \"score\": [70, 80, 90, 85]\n",
    "})\n",
    "\n",
    "stat_ob = Analyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "blNcm_VbtZV7",
   "metadata": {
    "id": "blNcm_VbtZV7"
   },
   "outputs": [],
   "source": [
    "stat_ob.fit(x, df[\"score\"], paired=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "pCgBtykmtsLu",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCgBtykmtsLu",
    "outputId": "a2b738ba-a4a7-45ab-a585-29f7e8f6714a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x_shape': (4,),\n",
       " 'y_shape': (4,),\n",
       " 'x_dtype': dtype('int64'),\n",
       " 'y_dtype': dtype('int64')}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stat_ob.summary_check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "R9dQ-XTLujSr",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "R9dQ-XTLujSr",
    "outputId": "e2332604-4c44-4b64-8361-bde89973b740"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chosen test: paired_ttest\n"
     ]
    }
   ],
   "source": [
    "chosen_test = stat_ob.analysis_starter()\n",
    "print(\"Chosen test:\", chosen_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X7_sRaxnEmN4",
   "metadata": {
    "id": "X7_sRaxnEmN4"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
