{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "fc7f71cb",
      "metadata": {
        "id": "fc7f71cb"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "import warnings\n",
        "from scipy.stats import chi2_contingency\n",
        "\n",
        "warnings.filterwarnings('ignore')\n",
        "sns.set_style(\"whitegrid\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "M2Sjjlx7vfkE"
      },
      "outputs": [],
      "source": [
        "class Analyzer:\n",
        "    def __init__(self):\n",
        "        self.A = None\n",
        "        self.B = None\n",
        "        self.paired = None\n",
        "        self.testtype = None\n",
        "        self.result = None\n",
        "\n",
        "    def format_prepare(self, data, column=None):\n",
        "        \"\"\"\n",
        "            This method should check for the data to be of right format.\n",
        "            For example if the data is a DataFrame, it should change the data to Series or numpy array.\n",
        "        \"\"\"\n",
        "        if isinstance(data, np.ndarray):\n",
        "            if data.ndim > 1:\n",
        "                return data.flatten()\n",
        "            return data\n",
        "\n",
        "        elif isinstance(data, pd.Series):\n",
        "            return data.values\n",
        "\n",
        "        elif isinstance(data, pd.DataFrame):\n",
        "            if column is None:\n",
        "                if data.shape[1] == 1:\n",
        "                    return data.iloc[:, 0].values\n",
        "                else:\n",
        "                    raise ValueError(\"DataFrame has multiple columns. Please specify 'column'.\")\n",
        "            else:\n",
        "                if column not in data.columns:\n",
        "                    raise ValueError(f\"Column '{column}' not found in DataFrame.\")\n",
        "                return data[column].values\n",
        "        elif isinstance(data, list):\n",
        "            return np.array(data)\n",
        "\n",
        "        else:\n",
        "            raise TypeError(\"Unsupported data type. Use list, numpy array, pandas Series, or DataFrame.\")\n",
        "\n",
        "\n",
        "    def fit(self, a, b, column_a=None, column_b=None, paired=False):\n",
        "        \"\"\"\n",
        "            This method assigns the data after fixing the format and check for their lengths.\n",
        "        \"\"\"\n",
        "        format_checked_a = self.format_prepare(a, column_a)\n",
        "        format_checked_b = self.format_prepare(b, column_b)\n",
        "\n",
        "        if len(format_checked_a) != len(format_checked_b):\n",
        "            raise ValueError(\"x and y must have the same length.\")\n",
        "        self.A = format_checked_a\n",
        "        self.B = format_checked_b\n",
        "        self.paired = paired\n",
        "\n",
        "    def summary_check(self):\n",
        "        return {\n",
        "            \"x_shape\": self.A.shape,\n",
        "            \"y_shape\": self.B.shape,\n",
        "            \"x_dtype\": self.A.dtype,\n",
        "            \"y_dtype\": self.B.dtype,\n",
        "            \"paired\": self.paired,\n",
        "            \"test type\": self.testtype\n",
        "        }\n",
        "    def _check_normality(self, data):\n",
        "        \"\"\"Check normality using Shapiro-Wilk test (p>0.05 = normal).\"\"\"\n",
        "        stat, p = stats.shapiro(data)\n",
        "        return p > 0.05\n",
        "\n",
        "    def analysis_starter(self, alpha=0.05):\n",
        "        \"\"\"\"\n",
        "            This method is the analysis starter. it has to check the data for normality and\n",
        "            also should distinguish between independent and paired data and chooses the test.\n",
        "        \"\"\"\n",
        "        if self.B is None:\n",
        "            if self._check_normality(self.B):\n",
        "                self.testtype = \"Pearson Correlation\"\n",
        "            else:\n",
        "                self.testtype = \"Spearman Correlation\"\n",
        "            return self.testtype\n",
        "\n",
        "        if self.paired:\n",
        "            if self._check_normality(self.A - self.B):\n",
        "                self.testtype = \"Paired t-test\"\n",
        "            else:\n",
        "                self.testtype = \"Wilcoxon Signed-Rank Test\"\n",
        "        else:\n",
        "            if self._check_normality(self.A) and self._check_normality(self.B):\n",
        "                var_equal = stats.levene(self.data1, self.data2).pvalue > 0.05\n",
        "                if var_equal:\n",
        "                    self.testtype = \"Independent t-test (equal var)\"\n",
        "                else:\n",
        "                    self.testtype = \"Welch t-test\"\n",
        "            else:\n",
        "                self.testtype = \"Mann-Whitney U Test\"\n",
        "\n",
        "    def run_test(self):\n",
        "        \"\"\"\n",
        "        Executes the statistical test based on self.testtype.\n",
        "        Saves the statistic and p-value as attributes.\n",
        "        \"\"\"\n",
        "        if self.testtype is None:\n",
        "            raise ValueError(\"Please run start_analysis() first to determine the test type.\")\n",
        "\n",
        "        elif self.testtype == \"Pearson Correlation\":\n",
        "            self.result = stats.pearsonr(self.A, self.A)\n",
        "        elif self.testtype == \"Spearman Correlation\":\n",
        "            self.result = stats.spearmanr(self.A, self.A)\n",
        "        elif self.testtype == \"Paired t-test\":\n",
        "            self.result = stats.ttest_rel(self.A, self.B)\n",
        "        elif self.testtype == \"Wilcoxon Signed-Rank Test\":\n",
        "            self.result = stats.wilcoxon(self.A, self.B)\n",
        "\n",
        "        elif self.testtype == \"Independent t-test (equal var)\":\n",
        "            self.result = stats.ttest_ind(self.A, self.B, equal_var=True)\n",
        "        elif self.testtype == \"Welch t-test\":\n",
        "            self.result = stats.ttest_ind(self.A, self.B, equal_var=False)\n",
        "        elif self.testtype == \"Mann-Whitney U Test\":\n",
        "            self.result = stats.mannwhitneyu(self.A, self.B)\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unsupported test type: {self.testtype}\")\n",
        "\n",
        "        return self.result\n",
        "\n",
        "\n",
        "    def interpret(self):\n",
        "        if self.A is None or self.B is None:\n",
        "            raise Exception('A and B are empty. You must specify data group a and b with object.fit(a, b).')\n",
        "        score, p_val = self.result\n",
        "        if p_val < 0.05:\n",
        "            print('Rejecting H0 hypothesis')\n",
        "        else:\n",
        "            print('Cannot reject H0.')\n",
        "\n",
        "\n",
        "# Effect size helpers (private)\n",
        "\n",
        "def _cohen_d_independent(self, a: np.ndarray, b: np.ndarray, equal_var: bool = True) -> float:\n",
        "    \"\"\"\n",
        "    Cohen's d for independent groups.\n",
        "    If equal_var=False (Welch), use the weighted pooled SD formula (still widely used).\n",
        "    \"\"\"\n",
        "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
        "    na, nb = len(a), len(b)\n",
        "    ma, mb = np.nanmean(a), np.nanmean(b)\n",
        "    sa, sb = np.nanstd(a, ddof=1), np.nanstd(b, ddof=1)\n",
        "\n",
        "    if equal_var:\n",
        "        # Pooled standard deviation\n",
        "        sp2 = ((na - 1) * sa**2 + (nb - 1) * sb**2) / (na + nb - 2)\n",
        "        sp = np.sqrt(sp2)\n",
        "        if sp == 0:\n",
        "            return 0.0\n",
        "        return (ma - mb) / sp\n",
        "    else:\n",
        "        # Glass delta variants exist; here use Hedges' g approximation with pooled via weights\n",
        "        # (common pragmatic choice when equal variance is not assumed)\n",
        "        wp = ((sa**2) / na + (sb**2) / nb)\n",
        "        sp = np.sqrt(((sa**2 + sb**2) / 2.0))\n",
        "        if sp == 0:\n",
        "            return 0.0\n",
        "        d = (ma - mb) / sp\n",
        "        # Optional small-sample correction (Hedges' g)\n",
        "        df = ( (sa**2/na + sb**2/nb)**2 ) / ( ((sa**2/na)**2)/(na-1) + ((sb**2/nb)**2)/(nb-1) )\n",
        "        J = 1 - (3/(4*df - 1)) if df > 1 else 1.0\n",
        "        return float(J * d)\n",
        "\n",
        "\n",
        "def _cohen_d_paired(self, a: np.ndarray, b: np.ndarray) -> float:\n",
        "    \"\"\"\n",
        "    Cohen's d for paired samples: mean(diff) / sd(diff).\n",
        "    \"\"\"\n",
        "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
        "    d = a - b\n",
        "    md = np.nanmean(d)\n",
        "    sd = np.nanstd(d, ddof=1)\n",
        "    if sd == 0:\n",
        "        return 0.0\n",
        "    return md / sd\n",
        "\n",
        "\n",
        "def _rank_biserial_from_mwu(self, a: np.ndarray, b: np.ndarray, u_stat: float) -> float:\n",
        "    \"\"\"\n",
        "    Rank-biserial correlation derived from Mannâ€“Whitney U:\n",
        "    r_rb = 1 - (2U) / (n_a * n_b)\n",
        "    Range: [-1, 1]\n",
        "    \"\"\"\n",
        "    na, nb = len(a), len(b)\n",
        "    if na == 0 or nb == 0:\n",
        "        return 0.0\n",
        "    return 1.0 - (2.0 * u_stat) / (na * nb)\n",
        "\n",
        "\n",
        "def _rank_biserial_from_wilcoxon(self, a: np.ndarray, b: np.ndarray, w_stat: float) -> float:\n",
        "    \"\"\"\n",
        "    Approximate rank-biserial for Wilcoxon signed-rank:\n",
        "    r_rb = (W_pos - W_neg) / (W_pos + W_neg)\n",
        "    We can approximate via: r_rb = 1 - (2 * W_neg) / (W_pos + W_neg)\n",
        "    SciPy returns the test statistic W = sum of signed ranks positive? Historically it's min(W+, W-).\n",
        "    Here we compute directly from diffs for clarity.\n",
        "    \"\"\"\n",
        "    a = np.asarray(a, dtype=float); b = np.asarray(b, dtype=float)\n",
        "    d = a - b\n",
        "    # Remove zeros (Wilcoxon ignores zeros)\n",
        "    d = d[d != 0]\n",
        "    if d.size == 0:\n",
        "        return 0.0\n",
        "\n",
        "    # Compute ranks of absolute differences\n",
        "    ranks = stats.rankdata(np.abs(d))\n",
        "    W_pos = np.sum(ranks[d > 0])\n",
        "    W_neg = np.sum(ranks[d < 0])\n",
        "    denom = (W_pos + W_neg)\n",
        "    if denom == 0:\n",
        "        return 0.0\n",
        "    return (W_pos - W_neg) / denom\n",
        "\n",
        "    #Part 3A: DataFrame management & safe access\n",
        "    def set_df(self, df: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Attach a working DataFrame to the Analyzer instance.\n",
        "        This DF will be used by univariate/bivariate helper methods.\n",
        "        \"\"\"\n",
        "        if not isinstance(df, pd.DataFrame):\n",
        "            raise TypeError(\"set_df expects a pandas DataFrame.\")\n",
        "        # you can set it even if __init__ didn't define it\n",
        "        self.df = df.copy()\n",
        "        return self\n",
        "\n",
        "    def _ensure_df(self):\n",
        "        \"\"\"\n",
        "        Ensure that a DataFrame has been set via set_df().\n",
        "        \"\"\"\n",
        "        if not hasattr(self, \"df\") or self.df is None:\n",
        "            raise AttributeError(\"No DataFrame set. Call set_df(df) first.\")\n",
        "\n",
        "    def _get_series(self, col: str, dropna: bool = True) -> pd.Series:\n",
        "        \"\"\"\n",
        "        Return a single column as a clean Series from self.df.\n",
        "        - Ensures the column exists\n",
        "        - Optionally drops NaNs\n",
        "        \"\"\"\n",
        "        self._ensure_df()\n",
        "        if col not in self.df.columns:\n",
        "            raise KeyError(f\"Column '{col}' not found in the DataFrame.\")\n",
        "        s = self.df[col]\n",
        "        if dropna:\n",
        "            s = s.dropna()\n",
        "        return s\n",
        "\n",
        "    # Part 2: interpretation & quick report\n",
        "    def _basic_group_stats(self) -> dict:\n",
        "        \"\"\"\n",
        "        Return simple descriptive stats for A and B (after dropping NaNs).\n",
        "        \"\"\"\n",
        "        if self.A is None or self.B is None:\n",
        "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
        "        a = np.asarray(self.A, dtype=float); a = a[~np.isnan(a)]\n",
        "        b = np.asarray(self.B, dtype=float); b = b[~np.isnan(b)]\n",
        "        stats_a = {\n",
        "            \"n\": int(len(a)),\n",
        "            \"mean\": float(np.nanmean(a)) if len(a) else np.nan,\n",
        "            \"std\": float(np.nanstd(a, ddof=1)) if len(a) > 1 else np.nan,\n",
        "            \"median\": float(np.nanmedian(a)) if len(a) else np.nan,\n",
        "        }\n",
        "        stats_b = {\n",
        "            \"n\": int(len(b)),\n",
        "            \"mean\": float(np.nanmean(b)) if len(b) else np.nan,\n",
        "            \"std\": float(np.nanstd(b, ddof=1)) if len(b) > 1 else np.nan,\n",
        "            \"median\": float(np.nanmedian(b)) if len(b) else np.nan,\n",
        "        }\n",
        "        return {\"A\": stats_a, \"B\": stats_b}\n",
        "\n",
        "    def _effect_size_label(self, value: float) -> str:\n",
        "        \"\"\"\n",
        "        Heuristic label for effect size magnitude.\n",
        "        Uses Cohen's d style thresholds (also reasonable for rank-biserial abs values).\n",
        "        \"\"\"\n",
        "        if value is None or np.isnan(value):\n",
        "            return \"unknown\"\n",
        "        v = abs(value)\n",
        "        if v >= 0.8: return \"large\"\n",
        "        if v >= 0.5: return \"medium\"\n",
        "        if v >= 0.2: return \"small\"\n",
        "        return \"very small\"\n",
        "\n",
        "    def interpret(self, alpha: float = 0.05, verbose: bool = True):\n",
        "        \"\"\"\n",
        "        Human-friendly interpretation of the latest test result.\n",
        "        If no result is present, runs the test first.\n",
        "        Prints a short narrative if verbose=True.\n",
        "        Returns the result dict augmented with group stats.\n",
        "        \"\"\"\n",
        "        if self.result is None:\n",
        "            # Ensure test type is selected and run the test\n",
        "            self.run_test(alpha=alpha)\n",
        "\n",
        "        res = dict(self.result)  # copy\n",
        "        groups = self._basic_group_stats()\n",
        "        res[\"group_stats\"] = groups\n",
        "\n",
        "        # Build a short message\n",
        "        test_name = res.get(\"test_used\", \"unknown test\")\n",
        "        stat = res.get(\"statistic\", np.nan)\n",
        "        p = res.get(\"p_value\", np.nan)\n",
        "        eff = res.get(\"effect_size\", None)\n",
        "        sig = res.get(\"significant\", False)\n",
        "\n",
        "        # Pick mean or median difference depending on parametric vs non-parametric\n",
        "        a_mean, b_mean = groups[\"A\"][\"mean\"], groups[\"B\"][\"mean\"]\n",
        "        a_med, b_med = groups[\"A\"][\"median\"], groups[\"B\"][\"median\"]\n",
        "        if isinstance(test_name, str) and any(k in test_name for k in [\"mannwhitney\", \"wilcoxon\"]):\n",
        "            diff = float(a_med - b_med)\n",
        "            diff_label = \"median(A) - median(B)\"\n",
        "        else:\n",
        "            diff = float(a_mean - b_mean)\n",
        "            diff_label = \"mean(A) - mean(B)\"\n",
        "\n",
        "        res[\"difference\"] = {\"name\": diff_label, \"value\": diff}\n",
        "        res[\"effect_label\"] = self._effect_size_label(eff)\n",
        "\n",
        "        if verbose:\n",
        "            print(\"=\" * 60)\n",
        "            print(f\"Test: {test_name} | alpha={alpha}\")\n",
        "            print(f\"Statistic={stat:.4f} | p-value={p:.4g} | significant={sig}\")\n",
        "            if eff is not None:\n",
        "                print(f\"Effect size={eff:.3f} ({res['effect_label']})\")\n",
        "            print(f\"{diff_label} = {diff:.4f}\")\n",
        "            print(f\"A: n={groups['A']['n']}, mean={groups['A']['mean']:.4f}, median={groups['A']['median']:.4f}, std={groups['A']['std']}\")\n",
        "            print(f\"B: n={groups['B']['n']}, mean={groups['B']['mean']:.4f}, median={groups['B']['median']:.4f}, std={groups['B']['std']}\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "        return res\n",
        "\n",
        "    def quick_report(self, alpha: float = 0.05) -> dict:\n",
        "        \"\"\"\n",
        "        One-call convenience: decide test (if needed), run it, and return a compact dict.\n",
        "        Does not print anything.\n",
        "        \"\"\"\n",
        "        if self.result is None or self.testtype is None:\n",
        "            self.analysis_starter(alpha=alpha)\n",
        "            self.run_test(alpha=alpha)\n",
        "\n",
        "        groups = self._basic_group_stats()\n",
        "        out = dict(self.result)\n",
        "        out[\"group_stats\"] = groups\n",
        "\n",
        "        # Also include a concise difference metric\n",
        "        test_name = out.get(\"test_used\", \"\")\n",
        "        if isinstance(test_name, str) and any(k in test_name for k in [\"mannwhitney\", \"wilcoxon\"]):\n",
        "            diff_name = \"median(A)-median(B)\"\n",
        "            diff_val = float(groups[\"A\"][\"median\"] - groups[\"B\"][\"median\"])\n",
        "        else:\n",
        "            diff_name = \"mean(A)-mean(B)\"\n",
        "            diff_val = float(groups[\"A\"][\"mean\"] - groups[\"B\"][\"mean\"])\n",
        "        out[\"difference\"] = {\"name\": diff_name, \"value\": diff_val}\n",
        "        out[\"effect_label\"] = self._effect_size_label(out.get(\"effect_size\", None))\n",
        "        return out\n",
        "\n",
        "    #  Part 3B: Univariate descriptions (use self.df)\n",
        "    def describe_numeric(self, col: str, bins: int = 10, kde: bool = True, alpha: float = 0.05):\n",
        "        \"\"\"\n",
        "        Univariate summary for a numeric column in self.df:\n",
        "        - descriptive stats, missing, Shapiro p-value, skew, IQR outliers\n",
        "        - plots: histogram(+KDE), boxplot, PDF/CDF\n",
        "        Returns a dict with the computed statistics.\n",
        "        \"\"\"\n",
        "        s = self._get_series(col, dropna=True).astype(float)\n",
        "\n",
        "        # Basic stats\n",
        "        desc = s.describe()\n",
        "        shapiro_p = stats.shapiro(s).pvalue if len(s) >= 3 else np.nan\n",
        "        is_normal = (shapiro_p > alpha) if not np.isnan(shapiro_p) else False\n",
        "        skew_val = stats.skew(s)\n",
        "        q1, q3 = s.quantile(0.25), s.quantile(0.75)\n",
        "        iqr = q3 - q1\n",
        "        lower_b, upper_b = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
        "        n_low = int((s < lower_b).sum()); n_high = int((s > upper_b).sum())\n",
        "        n_out = n_low + n_high\n",
        "        pct_out = 100.0 * n_out / len(s) if len(s) else 0.0\n",
        "        missing = int(self.df[col].isna().sum())\n",
        "\n",
        "        # --- plots ---\n",
        "        fig, ax = plt.subplots(2, 2, figsize=(16, 7), gridspec_kw={'height_ratios': (.85, .15)})\n",
        "\n",
        "        # Histogram (+ KDE)\n",
        "        sns.histplot(s, kde=kde, ax=ax[0, 0], color='#55A868')\n",
        "        ax[0, 0].set_title(f'Histogram of {col}')\n",
        "        ax[0, 0].set_xlabel('')\n",
        "        ax[0, 0].set_ylabel('Count')\n",
        "\n",
        "        # Boxplot\n",
        "        sns.boxplot(x=s, ax=ax[1, 0], color=\"#5583A8\", orient='h')\n",
        "        label_text = f\"Lower outliers: {n_low}\\nUpper outliers: {n_high}\\nTotal: {n_out} ({pct_out:.1f}%)\"\n",
        "        patch = mpatches.Patch(color='skyblue', label=label_text)\n",
        "        ax[1, 0].legend(handles=[patch], fontsize=10, loc='upper left', bbox_to_anchor=(1.02, 1))\n",
        "\n",
        "        # PDF/CDF\n",
        "        counts, bin_edges = np.histogram(s, bins=bins, density=True)\n",
        "        pdf = counts / counts.sum() if counts.sum() != 0 else counts\n",
        "        cdf = np.cumsum(pdf)\n",
        "        ax[1, 1] = plt.subplot(122)\n",
        "        plt.plot(bin_edges[1:], pdf, label='PDF')\n",
        "        plt.plot(bin_edges[1:], cdf, label='CDF')\n",
        "        plt.legend()\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # Tidy\n",
        "        ax[0, 0].set_xticklabels([])\n",
        "        ax[1, 0].set_yticklabels([])\n",
        "        fig.suptitle(col, fontsize=18)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        info = {\n",
        "            \"count\": int(desc.get(\"count\", 0)),\n",
        "            \"mean\": float(desc.get(\"mean\", np.nan)),\n",
        "            \"std\": float(desc.get(\"std\", np.nan)),\n",
        "            \"min\": float(desc.get(\"min\", np.nan)),\n",
        "            \"25%\": float(desc.get(\"25%\", np.nan)),\n",
        "            \"50%\": float(desc.get(\"50%\", np.nan)),\n",
        "            \"75%\": float(desc.get(\"75%\", np.nan)),\n",
        "            \"max\": float(desc.get(\"max\", np.nan)),\n",
        "            \"missing\": missing,\n",
        "            \"shapiro_p\": float(shapiro_p) if not np.isnan(shapiro_p) else None,\n",
        "            \"normal\": bool(is_normal),\n",
        "            \"skew\": float(skew_val),\n",
        "            \"iqr_lower_bound\": float(lower_b),\n",
        "            \"iqr_upper_bound\": float(upper_b),\n",
        "            \"outliers_lower\": n_low,\n",
        "            \"outliers_upper\": n_high,\n",
        "            \"outliers_total_pct\": round(pct_out, 2)\n",
        "        }\n",
        "        return info\n",
        "\n",
        "    def describe_categorical(self, col: str, top_n: int = None):\n",
        "        \"\"\"\n",
        "        Univariate summary for a categorical column in self.df:\n",
        "        - frequency table with percentages\n",
        "        - bar plot of counts (optionally top_n categories)\n",
        "        Returns a dict with counts and percents.\n",
        "        \"\"\"\n",
        "        s = self._get_series(col, dropna=False)  # keep NaN to report missing\n",
        "        counts = s.value_counts(dropna=True)\n",
        "        if top_n is not None and top_n > 0:\n",
        "            counts = counts.head(top_n)\n",
        "        total_non_na = counts.sum()\n",
        "        perc = (counts / total_non_na * 100.0).round(2) if total_non_na else counts\n",
        "\n",
        "        # Plot\n",
        "        plt.figure(figsize=(12, 5))\n",
        "        sns.barplot(x=counts.index.astype(str), y=counts.values, color=\"#55A868\")\n",
        "        plt.title(f'Counts of {col}')\n",
        "        plt.ylabel('Count')\n",
        "        plt.xlabel(col)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        info = {\n",
        "            \"missing\": int(s.isna().sum()),\n",
        "            \"unique\": int(s.nunique(dropna=True)),\n",
        "            \"counts\": counts.to_dict(),\n",
        "            \"percents\": perc.to_dict()\n",
        "        }\n",
        "        return info\n",
        "\n",
        "    def bin_numeric(self, col: str, bins, labels=None, new_col: str = None,\n",
        "                    right: bool = True, include_lowest: bool = True):\n",
        "        \"\"\"\n",
        "        Create categorical bins from a numeric column using pd.cut.\n",
        "        - bins: list/array of bin edges\n",
        "        - labels: optional labels for each bin\n",
        "        - new_col: optional name for the new column; defaults to f\"{col}_binned\"\n",
        "        Returns the created Series.\n",
        "        \"\"\"\n",
        "        self._ensure_df()\n",
        "        if new_col is None:\n",
        "            new_col = f\"{col}_binned\"\n",
        "        s = self._get_series(col, dropna=False)\n",
        "        binned = pd.cut(s, bins=bins, labels=labels, right=right, include_lowest=include_lowest)\n",
        "        self.df[new_col] = binned\n",
        "        return self.df[new_col]\n",
        "\n",
        "    #  Part 3C: Bivariate analyses (use self.df)\n",
        "    def rel(self, feature: str, target: str, alpha: float = 0.05):\n",
        "        \"\"\"\n",
        "        Dispatcher: choose the appropriate relationship method based on dtypes.\n",
        "        \"\"\"\n",
        "        x = self._get_series(feature, dropna=False)\n",
        "        y = self._get_series(target, dropna=False)\n",
        "\n",
        "        # Decide categorical vs numeric\n",
        "        x_is_cat = self._is_categorical(x)\n",
        "        y_is_cat = self._is_categorical(y)\n",
        "\n",
        "        if not x_is_cat and not y_is_cat:\n",
        "            return self.rel_num_num(feature, target, alpha=alpha)\n",
        "        elif x_is_cat and y_is_cat:\n",
        "            return self.rel_cat_cat(feature, target, alpha=alpha)\n",
        "        else:\n",
        "            # Ensure (cat, num) order\n",
        "            if x_is_cat and not y_is_cat:\n",
        "                return self.rel_cat_num(cat_col=feature, num_col=target, alpha=alpha)\n",
        "            else:\n",
        "                return self.rel_cat_num(cat_col=target, num_col=feature, alpha=alpha)\n",
        "\n",
        "    def rel_num_num(self, col1: str, col2: str, alpha: float = 0.05, gridsize: int = 20):\n",
        "        \"\"\"\n",
        "        Numeric vs Numeric:\n",
        "        - Normality via Shapiro (if len>=3)\n",
        "        - Pearson if both normal else Spearman\n",
        "        - Plots: scatter + hexbin\n",
        "        Returns dict with test info.\n",
        "        \"\"\"\n",
        "        s1 = self._get_series(col1, dropna=True).astype(float)\n",
        "        s2 = self._get_series(col2, dropna=True).astype(float)\n",
        "\n",
        "        # align indices (just in case lengths differ after dropna)\n",
        "        df_pair = pd.DataFrame({col1: s1, col2: s2}).dropna()\n",
        "        x = df_pair[col1].values\n",
        "        y = df_pair[col2].values\n",
        "\n",
        "        sh1 = stats.shapiro(x).pvalue if len(x) >= 3 else np.nan\n",
        "        sh2 = stats.shapiro(y).pvalue if len(y) >= 3 else np.nan\n",
        "        both_normal = (sh1 > alpha if not np.isnan(sh1) else False) and (sh2 > alpha if not np.isnan(sh2) else False)\n",
        "\n",
        "        if both_normal:\n",
        "            test_used = \"Pearson\"\n",
        "            r, p = stats.pearsonr(x, y)\n",
        "        else:\n",
        "            test_used = \"Spearman\"\n",
        "            r, p = stats.spearmanr(x, y)\n",
        "\n",
        "        r2 = r ** 2\n",
        "        strength = \"strong\" if abs(r) > 0.7 else (\"moderate\" if abs(r) > 0.3 else \"weak\")\n",
        "        direction = \"positive\" if r > 0 else \"negative\"\n",
        "\n",
        "        # --- plots ---\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
        "        fig.suptitle(f'{col1} vs {col2} - {test_used} correlation', fontsize=14)\n",
        "\n",
        "        # Scatter\n",
        "        axes[0].scatter(x, y, s=50, alpha=0.7, color='blue', edgecolors='black', linewidth=0.5)\n",
        "        axes[0].set_title('Scatter')\n",
        "        axes[0].set_xlabel(col1); axes[0].set_ylabel(col2); axes[0].grid(True, alpha=0.3)\n",
        "\n",
        "        # Hexbin\n",
        "        hb = axes[1].hexbin(x, y, gridsize=gridsize, cmap='Blues', mincnt=1)\n",
        "        axes[1].set_title('Hexbin'); axes[1].set_xlabel(col1); axes[1].set_ylabel(col2)\n",
        "        cb = fig.colorbar(hb, ax=axes[1]); cb.set_label('count')\n",
        "\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        return {\n",
        "            \"test_used\": test_used,\n",
        "            \"shapiro_p_col1\": float(sh1) if not np.isnan(sh1) else None,\n",
        "            \"shapiro_p_col2\": float(sh2) if not np.isnan(sh2) else None,\n",
        "            \"correlation\": float(r),\n",
        "            \"r_squared\": float(r2),\n",
        "            \"p_value\": float(p),\n",
        "            \"significant\": bool(p < alpha),\n",
        "            \"relationship_strength\": strength,\n",
        "            \"relationship_direction\": direction,\n",
        "            \"n\": int(len(x))\n",
        "        }\n",
        "\n",
        "    def rel_cat_num(self, cat_col: str, num_col: str, alpha: float = 0.05):\n",
        "        \"\"\"\n",
        "        Categorical vs Numeric:\n",
        "        - Shapiro per group; if all normal -> ANOVA; else Welch's ANOVA (if available) else Kruskal-Wallis\n",
        "        - Plots: box, violin, mean bar, count of categories\n",
        "        Returns dict with test info + per-group normality.\n",
        "        \"\"\"\n",
        "        s_cat = self._get_series(cat_col, dropna=False)\n",
        "        s_num = self._get_series(num_col, dropna=True).astype(float)\n",
        "\n",
        "        # Align and drop NaNs in numeric and category simultaneously\n",
        "        df_local = pd.DataFrame({cat_col: s_cat, num_col: self.df[num_col]}).dropna()\n",
        "        # Re-extract arrays\n",
        "        groups = df_local.groupby(cat_col)[num_col].describe()\n",
        "        normality = {}\n",
        "        data_groups = []\n",
        "        all_normal = True\n",
        "\n",
        "        for name, grp in df_local.groupby(cat_col):\n",
        "            vals = grp[num_col].values\n",
        "            if len(vals) >= 3:\n",
        "                p_sh = stats.shapiro(vals).pvalue\n",
        "                is_norm = p_sh > alpha\n",
        "                normality[name] = {\"shapiro_p\": float(p_sh), \"is_normal\": bool(is_norm), \"n\": int(len(vals))}\n",
        "                if not is_norm: all_normal = False\n",
        "            else:\n",
        "                normality[name] = {\"shapiro_p\": None, \"is_normal\": False, \"n\": int(len(vals))}\n",
        "                all_normal = False\n",
        "            data_groups.append(vals)\n",
        "\n",
        "        # Choose test\n",
        "        test_used = None\n",
        "        stat, p_val = np.nan, np.nan\n",
        "        if len(data_groups) >= 2:\n",
        "            if all_normal:\n",
        "                test_used = \"ANOVA\"\n",
        "                stat, p_val = stats.f_oneway(*data_groups)\n",
        "            else:\n",
        "                # Try Alexander-Govern (Welch's ANOVA in SciPy name) if available\n",
        "                try:\n",
        "                    from scipy.stats import alexandergovern\n",
        "                    result = alexandergovern(*data_groups)\n",
        "                    stat, p_val = float(result.statistic), float(result.pvalue)\n",
        "                    test_used = \"Welch_ANOVA\"\n",
        "                except Exception:\n",
        "                    stat, p_val = stats.kruskal(*data_groups)\n",
        "                    test_used = \"Kruskal_Wallis\"\n",
        "        else:\n",
        "            test_used = \"insufficient_groups\"\n",
        "\n",
        "        # --- plots ---\n",
        "        fig = plt.figure(figsize=(14, 10))\n",
        "\n",
        "        ax1 = plt.subplot(221)\n",
        "        sns.boxplot(data=df_local, x=cat_col, y=num_col, ax=ax1)\n",
        "        ax1.set_title(f'{num_col} by {cat_col}'); plt.xticks(rotation=45)\n",
        "\n",
        "        ax2 = plt.subplot(222)\n",
        "        sns.violinplot(data=df_local, x=cat_col, y=num_col, ax=ax2)\n",
        "        ax2.set_title(f'{num_col} distribution by {cat_col}'); plt.xticks(rotation=45)\n",
        "\n",
        "        ax3 = plt.subplot(223)\n",
        "        means = df_local.groupby(cat_col)[num_col].mean().sort_values(ascending=False)\n",
        "        sns.barplot(x=means.index, y=means.values, ax=ax3)\n",
        "        ax3.set_title(f'Mean {num_col} by {cat_col}'); plt.xticks(rotation=45)\n",
        "\n",
        "        ax4 = plt.subplot(224)\n",
        "        sns.countplot(data=df_local, x=cat_col, ax=ax4)\n",
        "        ax4.set_title(f'Count of {cat_col}'); plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        return {\n",
        "            \"test_used\": test_used,\n",
        "            \"all_groups_normal\": bool(all_normal),\n",
        "            \"statistic\": float(stat) if not np.isnan(stat) else None,\n",
        "            \"p_value\": float(p_val) if not np.isnan(p_val) else None,\n",
        "            \"significant\": (p_val < alpha) if not np.isnan(p_val) else None,\n",
        "            \"num_categories\": int(df_local[cat_col].nunique()),\n",
        "            \"total_observations\": int(len(df_local)),\n",
        "            \"group_stats\": groups.to_dict(),\n",
        "            \"normality\": normality\n",
        "        }\n",
        "\n",
        "    def rel_cat_cat(self, col1: str, col2: str, alpha: float = 0.05):\n",
        "        \"\"\"\n",
        "        Categorical vs Categorical:\n",
        "        - Contingency table + Chi-square test\n",
        "        - Effect size: CramÃ©r's V\n",
        "        - Plots: heatmap (counts), stacked 100% bar, normalized heatmap, grouped counts\n",
        "        Returns dict with chi2, p, dof, CramÃ©r's V.\n",
        "        \"\"\"\n",
        "        s1 = self._get_series(col1, dropna=False).astype(\"category\")\n",
        "        s2 = self._get_series(col2, dropna=False).astype(\"category\")\n",
        "\n",
        "        df_local = pd.DataFrame({col1: s1, col2: s2}).dropna()\n",
        "        table = pd.crosstab(df_local[col1], df_local[col2])\n",
        "        norm = pd.crosstab(df_local[col1], df_local[col2], normalize='index')\n",
        "        n = table.values.sum()\n",
        "\n",
        "        # Chi-square test\n",
        "        chi2, p_val, dof, expected = stats.chi2_contingency(table)\n",
        "\n",
        "        # Check expected counts rule-of-thumb\n",
        "        too_small = (expected < 5).sum()\n",
        "        expected_ok = (too_small / expected.size) <= 0.2  # no more than 20% < 5\n",
        "\n",
        "        # CramÃ©r's V\n",
        "        k = min(table.shape) - 1\n",
        "        cramers_v = np.sqrt(chi2 / (n * max(k, 1)))\n",
        "\n",
        "        strength = \"strong\" if cramers_v > 0.3 else (\"moderate\" if cramers_v > 0.1 else \"weak\")\n",
        "\n",
        "        # --- plots ---\n",
        "        fig, axs = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "        sns.heatmap(table, annot=True, fmt='d', cbar=False, cmap='YlGnBu',\n",
        "                    ax=axs[0, 0], linecolor='lightgray', linewidths=0.7)\n",
        "        axs[0, 0].set_title(f'Contingency: {col1} vs {col2}')\n",
        "\n",
        "        norm.plot.bar(stacked=True, ax=axs[0, 1])\n",
        "        axs[0, 1].set_title('Stacked Bar (100%)')\n",
        "        axs[0, 1].legend(title=col2)\n",
        "        axs[0, 1].set_xlabel(col1); axs[0, 1].set_ylabel('Proportion')\n",
        "\n",
        "        sns.heatmap(norm, annot=True, fmt='.2%', cbar=False, cmap='YlGnBu',\n",
        "                    ax=axs[1, 0], linecolor='lightgray', linewidths=0.7)\n",
        "        axs[1, 0].set_title('Row-normalized (%)')\n",
        "\n",
        "        table.plot(kind='bar', ax=axs[1, 1])\n",
        "        axs[1, 1].set_title('Counts by group'); axs[1, 1].legend(title=col2)\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        plt.tight_layout(); plt.show()\n",
        "\n",
        "        return {\n",
        "            \"chi2_statistic\": float(chi2),\n",
        "            \"p_value\": float(p_val),\n",
        "            \"degrees_of_freedom\": int(dof),\n",
        "            \"cramers_v\": float(cramers_v),\n",
        "            \"association_strength\": strength,\n",
        "            \"significant_association\": bool(p_val < alpha),\n",
        "            \"expected_counts_ok\": bool(expected_ok),\n",
        "            \"contingency_table\": table.to_dict(),\n",
        "            \"normalized_row\": norm.to_dict(),\n",
        "            \"n\": int(n)\n",
        "        }\n",
        "\n",
        "    #  Part 3D: KPI helpers & plots (use self.df)\n",
        "    def _get_grouped_data(self, group_cols, kpi: str = \"Conversion Rate\"):\n",
        "        \"\"\"\n",
        "        Internal helper: group self.df by group_cols and compute KPI.\n",
        "        Supported KPIs:\n",
        "          - \"Conversion Rate\" = conversions / clicks * 100\n",
        "          - \"CPC\"             = cost / clicks\n",
        "        Expected raw columns (if you want auto-compute):\n",
        "          - 'clicks', 'conversions', 'cost'\n",
        "        If KPI column already exists in self.df, grouping will aggregate by mean.\n",
        "        Returns a grouped DataFrame with columns: group_cols + [kpi]\n",
        "        \"\"\"\n",
        "        self._ensure_df()\n",
        "        if isinstance(group_cols, (str,)):\n",
        "            group_cols = [group_cols]\n",
        "\n",
        "        df = self.df.copy()\n",
        "\n",
        "        # If KPI column missing, try to compute from standard raw columns\n",
        "        if kpi not in df.columns:\n",
        "            # Auto-compute if raw columns exist\n",
        "            if kpi == \"Conversion Rate\":\n",
        "                if all(c in df.columns for c in [\"conversions\", \"clicks\"]):\n",
        "                    # Avoid division by zero\n",
        "                    df[\"Conversion Rate\"] = np.where(df[\"clicks\"] > 0,\n",
        "                                                     (df[\"conversions\"] / df[\"clicks\"]) * 100.0, np.nan)\n",
        "                else:\n",
        "                    raise KeyError(\"To compute 'Conversion Rate', columns 'conversions' and 'clicks' must exist.\")\n",
        "            elif kpi == \"CPC\":\n",
        "                if all(c in df.columns for c in [\"cost\", \"clicks\"]):\n",
        "                    df[\"CPC\"] = np.where(df[\"clicks\"] > 0, df[\"cost\"] / df[\"clicks\"], np.nan)\n",
        "                else:\n",
        "                    raise KeyError(\"To compute 'CPC', columns 'cost' and 'clicks' must exist.\")\n",
        "            else:\n",
        "                raise ValueError(\"KPI must be either 'Conversion Rate' or 'CPC'.\")\n",
        "\n",
        "        # Group and aggregate KPI by mean (typical for rates/cost per click after per-row calc)\n",
        "        grouped = df.groupby(group_cols, dropna=False, as_index=False)[kpi].mean()\n",
        "        return grouped\n",
        "\n",
        "    def kpi_heatmap(self, group1: str, group2: str, kpi: str = \"Conversion Rate\"):\n",
        "        \"\"\"\n",
        "        Draw a heatmap of KPI by two categorical dimensions.\n",
        "        \"\"\"\n",
        "        if kpi not in [\"Conversion Rate\", \"CPC\"]:\n",
        "            raise ValueError('KPI must be \"Conversion Rate\" or \"CPC\"')\n",
        "\n",
        "        tmp = self._get_grouped_data([group1, group2], kpi=kpi)\n",
        "\n",
        "        heatmap_data = tmp.pivot_table(index=group1, columns=group2, values=kpi)\n",
        "        heatmap_data = heatmap_data.fillna(0)\n",
        "\n",
        "        plt.figure(figsize=(18, 8))\n",
        "        sns.heatmap(\n",
        "            heatmap_data,\n",
        "            cmap='YlGnBu',\n",
        "            annot=True,\n",
        "            linewidths=0.7,\n",
        "            linecolor='lightgray'\n",
        "        )\n",
        "        plt.title(f'Heatmap of {kpi} by {group1} / {group2}', fontsize=18, weight='bold')\n",
        "        plt.ylabel(group1); plt.xlabel(group2)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return heatmap_data  # return matrix for possible reuse\n",
        "\n",
        "    def kpi_bar(self, group_col: str, kpi: str = \"Conversion Rate\"):\n",
        "        \"\"\"\n",
        "        Draw a barplot of KPI by a single categorical dimension and annotate bars.\n",
        "        \"\"\"\n",
        "        if kpi not in [\"Conversion Rate\", \"CPC\"]:\n",
        "            raise ValueError('KPI must be \"Conversion Rate\" or \"CPC\"')\n",
        "\n",
        "        tmp = self._get_grouped_data(group_col, kpi=kpi)\n",
        "\n",
        "        plt.figure(figsize=(12, 7))\n",
        "        ax = sns.barplot(data=tmp, x=group_col, y=kpi, palette='viridis')\n",
        "        ax.set_title(f'{kpi} by {group_col}')\n",
        "        ax.set_xlabel(group_col); ax.set_ylabel(kpi)\n",
        "        plt.xticks(rotation=45, ha='right')\n",
        "\n",
        "        # Annotate each bar with a friendly label\n",
        "        for p in ax.patches:\n",
        "            val = p.get_height()\n",
        "            if np.isnan(val):\n",
        "                label = \"NA\"\n",
        "            else:\n",
        "                label = f'{val:.2f}%' if kpi == \"Conversion Rate\" else f'{val:.4f}$'\n",
        "            ax.annotate(label,\n",
        "                        (p.get_x() + p.get_width()/2., val if not np.isnan(val) else 0),\n",
        "                        ha='center', va='center', fontsize=11, color='black',\n",
        "                        xytext=(0, 10), textcoords='offset points')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        return tmp  # return grouped table\n",
        "\n",
        "    def visual_groups(self, bins: int = 20, kde: bool = True, title: str = None):\n",
        "        \"\"\"\n",
        "        Visual comparison of groups A and B:\n",
        "        - overlaid histograms (+optional KDE)\n",
        "        - side-by-side boxplots\n",
        "        - side-by-side violin plots\n",
        "        - Qâ€“Q plots vs Normal\n",
        "        Requirements: self.A and self.B set via fit().\n",
        "        \"\"\"\n",
        "        if self.A is None or self.B is None:\n",
        "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
        "\n",
        "        a = np.asarray(self.A, dtype=float)\n",
        "        b = np.asarray(self.B, dtype=float)\n",
        "        a = a[~np.isnan(a)]\n",
        "        b = b[~np.isnan(b)]\n",
        "\n",
        "        if len(a) == 0 or len(b) == 0:\n",
        "            raise ValueError(\"A and/or B contain no valid numeric values after NaN removal.\")\n",
        "\n",
        "        # --- Figure layout ---\n",
        "        fig = plt.figure(figsize=(16, 10))\n",
        "        if title is None:\n",
        "            title = \"A vs B â€” distribution & diagnostics\"\n",
        "        fig.suptitle(title, fontsize=16, y=0.98)\n",
        "\n",
        "        # 1) Overlaid histograms (+KDE)\n",
        "        ax1 = plt.subplot(221)\n",
        "        sns.histplot(a, bins=bins, kde=kde, stat='density', color=\"#4C78A8\", alpha=0.45, ax=ax1, label=\"A\")\n",
        "        sns.histplot(b, bins=bins, kde=kde, stat='density', color=\"#F58518\", alpha=0.45, ax=ax1, label=\"B\")\n",
        "        ax1.set_title(\"Overlaid histograms\")\n",
        "        ax1.set_xlabel(\"\"); ax1.set_ylabel(\"Density\")\n",
        "        ax1.legend()\n",
        "\n",
        "        # 2) Boxplots (side-by-side)\n",
        "        ax2 = plt.subplot(222)\n",
        "        sns.boxplot(data=[a, b], orient='v', ax=ax2, palette=[\"#4C78A8\", \"#F58518\"])\n",
        "        ax2.set_title(\"Boxplots (A, B)\")\n",
        "        ax2.set_xticklabels([\"A\", \"B\"])\n",
        "\n",
        "        # 3) Violin plots (side-by-side)\n",
        "        ax3 = plt.subplot(223)\n",
        "        # Build a compact DataFrame for seaborn violin\n",
        "        df_tmp = pd.DataFrame({\n",
        "            \"value\": np.r_[a, b],\n",
        "            \"group\": [\"A\"] * len(a) + [\"B\"] * len(b)\n",
        "        })\n",
        "        sns.violinplot(data=df_tmp, x=\"group\", y=\"value\", ax=ax3, palette=[\"#4C78A8\", \"#F58518\"])\n",
        "        ax3.set_title(\"Violin plots\")\n",
        "\n",
        "        # 4) Qâ€“Q plots against Normal\n",
        "        ax4 = plt.subplot(224)\n",
        "        # For readability, draw QQ for both A and B sequentially\n",
        "        stats.probplot(a, dist=\"norm\", plot=ax4)\n",
        "        stats.probplot(b, dist=\"norm\", plot=ax4)\n",
        "        ax4.set_title(\"Qâ€“Q plots vs Normal\")\n",
        "\n",
        "        plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
        "        plt.show()\n",
        "\n",
        "    def visual_groups_pdf_cdf(self, bins: int = 20):\n",
        "        \"\"\"\n",
        "        Plot PDF/CDF curves for groups A and B in a single axes pair.\n",
        "        Good for comparing cumulative behavior (stochastic dominance).\n",
        "        \"\"\"\n",
        "        if self.A is None or self.B is None:\n",
        "            raise Exception(\"A and/or B are empty. Call fit(a, b, ...) first.\")\n",
        "        a = np.asarray(self.A, dtype=float); a = a[~np.isnan(a)]\n",
        "        b = np.asarray(self.B, dtype=float); b = b[~np.isnan(b)]\n",
        "\n",
        "        if len(a) == 0 or len(b) == 0:\n",
        "            raise ValueError(\"A and/or B contain no valid numeric values after NaN removal.\")\n",
        "\n",
        "        # Compute PDF/CDF for A\n",
        "        ca, ba = np.histogram(a, bins=bins, density=True)\n",
        "        pdf_a = ca / ca.sum() if ca.sum() != 0 else ca\n",
        "        cdf_a = np.cumsum(pdf_a)\n",
        "\n",
        "        # Compute PDF/CDF for B\n",
        "        cb, bb = np.histogram(b, bins=bins, density=True)\n",
        "        pdf_b = cb / cb.sum() if cb.sum() != 0 else cb\n",
        "        cdf_b = np.cumsum(pdf_b)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "        # PDF\n",
        "        axes[0].plot(ba[1:], pdf_a, label=\"A - PDF\")\n",
        "        axes[0].plot(bb[1:], pdf_b, label=\"B - PDF\")\n",
        "        axes[0].set_title(\"PDF comparison\"); axes[0].legend()\n",
        "\n",
        "        # CDF\n",
        "        axes[1].plot(ba[1:], cdf_a, label=\"A - CDF\")\n",
        "        axes[1].plot(bb[1:], cdf_b, label=\"B - CDF\")\n",
        "        axes[1].set_title(\"CDF comparison\"); axes[1].legend()\n",
        "\n",
        "        for ax in axes:\n",
        "            ax.grid(True, alpha=0.25)\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    #  Part 5: final wiring & cleanup\n",
        "    def visual_description(self, bins: int = 20, kde: bool = True, title: str = None):\n",
        "        \"\"\"\n",
        "        Quick visual overview for groups A and B.\n",
        "        Shows overlaid histograms, box/violin, and QQ plots.\n",
        "        \"\"\"\n",
        "        self.visual_groups(bins=bins, kde=kde, title=title)\n",
        "\n",
        "    # (optional) keep legacy name but warn; not used anymore\n",
        "    def __initial_test(self, test):\n",
        "        \"\"\"\n",
        "        DEPRECATED: kept only for backward compatibility.\n",
        "        Use analysis_starter() + run_test() instead.\n",
        "        \"\"\"\n",
        "        raise DeprecationWarning(\n",
        "            \"Use analysis_starter() to choose the test, then run_test() to execute it.\"\n",
        "        )"
      ],
      "id": "M2Sjjlx7vfkE"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "67313a55",
      "metadata": {
        "id": "67313a55"
      },
      "outputs": [],
      "source": [
        "x1 = [22, 23, 24, 25]\n",
        "df = df = pd.DataFrame({\n",
        "    \"age\": [20, 22, 25, 30],\n",
        "    \"score\": [70, 80, 90, 85]\n",
        "})\n",
        "x = [12, 15, 14, 10, 13, 15, 16]\n",
        "y = [2, 2, 2, 2, 2, 6, 8]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stat_ob = Analyzer()\n",
        "stat_ob.fit(x1, df[\"score\"], paired=True)\n",
        "stat_ob.analysis_starter()"
      ],
      "metadata": {
        "id": "blNcm_VbtZV7"
      },
      "id": "blNcm_VbtZV7",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer_test = Analyzer()\n",
        "analyzer_test.fit(x, y, paired=False)\n",
        "analyzer_test.analysis_starter()\n"
      ],
      "metadata": {
        "id": "zgUDk40_0ncS"
      },
      "id": "zgUDk40_0ncS",
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stat_ob.summary_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCgBtykmtsLu",
        "outputId": "06e0fb12-ee1b-484d-9fc6-f92b7c23398e"
      },
      "id": "pCgBtykmtsLu",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x_shape': (4,),\n",
              " 'y_shape': (4,),\n",
              " 'x_dtype': dtype('int64'),\n",
              " 'y_dtype': dtype('int64'),\n",
              " 'paired': True,\n",
              " 'test type': 'Paired t-test'}"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "analyzer_test.summary_check()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jHRN4eo-7j6R",
        "outputId": "2051f057-02c0-45f8-c02b-022915876dcc"
      },
      "id": "jHRN4eo-7j6R",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'x_shape': (7,),\n",
              " 'y_shape': (7,),\n",
              " 'x_dtype': dtype('int64'),\n",
              " 'y_dtype': dtype('int64'),\n",
              " 'paired': False,\n",
              " 'test type': 'Mann-Whitney U Test'}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M7kyUDPv7rXN"
      },
      "id": "M7kyUDPv7rXN",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "QBX",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.23"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}